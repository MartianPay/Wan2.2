# Wan2.2 GPU é›†ç¾¤ç”Ÿäº§éƒ¨ç½²å®Œæ•´æ–¹æ¡ˆ

> **æ–‡æ¡£ç‰ˆæœ¬**: v1.0
> **æ›´æ–°æ—¥æœŸ**: 2025-11-03
> **é€‚ç”¨åœºæ™¯**: Text-to-Video / Image-to-Video å¤§è§„æ¨¡æœåŠ¡åŒ–éƒ¨ç½²

---

## ğŸ“‹ ç›®å½•

- [1. æ–¹æ¡ˆæ¦‚è¿°](#1-æ–¹æ¡ˆæ¦‚è¿°)
- [2. æ¶æ„è®¾è®¡](#2-æ¶æ„è®¾è®¡)
- [3. æŠ€æœ¯æ ˆé€‰å‹](#3-æŠ€æœ¯æ ˆé€‰å‹)
- [4. å¼€æºGPUé›†ç¾¤ç®¡ç†æ–¹æ¡ˆå¯¹æ¯”](#4-å¼€æºgpué›†ç¾¤ç®¡ç†æ–¹æ¡ˆå¯¹æ¯”)
- [5. æ¨èæ–¹æ¡ˆï¼šKubernetes + Redis](#5-æ¨èæ–¹æ¡ˆkubernetes--redis)
- [6. å®Œæ•´éƒ¨ç½²æ­¥éª¤](#6-å®Œæ•´éƒ¨ç½²æ­¥éª¤)
- [7. API ä½¿ç”¨ç¤ºä¾‹](#7-api-ä½¿ç”¨ç¤ºä¾‹)
- [8. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥](#8-æ€§èƒ½ä¼˜åŒ–ç­–ç•¥)
- [9. ç›‘æ§ä¸å‘Šè­¦](#9-ç›‘æ§ä¸å‘Šè­¦)
- [10. æˆæœ¬ä¼˜åŒ–](#10-æˆæœ¬ä¼˜åŒ–)
- [11. å®‰å…¨æœ€ä½³å®è·µ](#11-å®‰å…¨æœ€ä½³å®è·µ)
- [12. å¸¸è§é—®é¢˜FAQ](#12-å¸¸è§é—®é¢˜faq)
- [13. é™„å½•ï¼šå®Œæ•´ä»£ç ](#13-é™„å½•å®Œæ•´ä»£ç )

---

## 1. æ–¹æ¡ˆæ¦‚è¿°

### 1.1 ä¸šåŠ¡éœ€æ±‚

æ„å»ºä¸€ä¸ªé«˜æ€§èƒ½ã€é«˜å¯ç”¨çš„ GPU é›†ç¾¤ï¼Œç”¨äºå¤„ç†ç”¨æˆ·æäº¤çš„è§†é¢‘ç”Ÿæˆä»»åŠ¡ï¼š

- **Text-to-Video (T2V)**: æ–‡æœ¬ç”Ÿæˆè§†é¢‘
- **Image-to-Video (I2V)**: å›¾ç‰‡ç”Ÿæˆè§†é¢‘
- **Text-Image-to-Video (TI2V)**: æ–‡æœ¬+å›¾ç‰‡ç”Ÿæˆè§†é¢‘
- **Speech-to-Video (S2V)**: è¯­éŸ³ç”Ÿæˆè§†é¢‘
- **Character Animation**: è§’è‰²åŠ¨ç”»ç”Ÿæˆ

### 1.2 æ ¸å¿ƒç›®æ ‡

| ç›®æ ‡ | æŒ‡æ ‡ |
|------|------|
| **é«˜æ€§èƒ½** | å•ä»»åŠ¡ < 5åˆ†é’Ÿï¼Œååé‡ > 100 è§†é¢‘/å°æ—¶ |
| **é«˜å¯ç”¨** | æœåŠ¡å¯ç”¨æ€§ > 99.9% |
| **å¼¹æ€§æ‰©å±•** | æ ¹æ®è´Ÿè½½è‡ªåŠ¨æ‰©ç¼©å®¹ (1-50 GPU) |
| **æˆæœ¬ä¼˜åŒ–** | ä½¿ç”¨ Spot å®ä¾‹èŠ‚çœ 60-70% æˆæœ¬ |
| **æ˜“ç»´æŠ¤** | æ ‡å‡†åŒ–éƒ¨ç½²ï¼Œè‡ªåŠ¨åŒ–è¿ç»´ |

### 1.3 æŠ€æœ¯æŒ‘æˆ˜

1. **GPU èµ„æºè°ƒåº¦**: å¦‚ä½•é«˜æ•ˆåˆ†é…å’Œç®¡ç†å¤šä¸ª GPU èŠ‚ç‚¹
2. **ä»»åŠ¡é˜Ÿåˆ—ç®¡ç†**: é«˜å¹¶å‘ä»»åŠ¡çš„æ’é˜Ÿã€åˆ†å‘ã€é‡è¯•
3. **æ¨¡å‹åŠ è½½ä¼˜åŒ–**: é¿å…é‡å¤åŠ è½½ï¼Œå‡å°‘å†·å¯åŠ¨æ—¶é—´
4. **å†…å­˜ç®¡ç†**: é˜²æ­¢ GPU OOMï¼Œåˆç†åˆ©ç”¨ VRAM
5. **æˆæœ¬æ§åˆ¶**: åœ¨æ€§èƒ½å’Œæˆæœ¬é—´æ‰¾åˆ°å¹³è¡¡ç‚¹

---

## 2. æ¶æ„è®¾è®¡

### 2.1 æ€»ä½“æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       ç”¨æˆ·/å®¢æˆ·ç«¯åº”ç”¨                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“ HTTPS
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              AWS Application Load Balancer (ALB)                 â”‚
â”‚  - SSL ç»ˆæ­¢                                                       â”‚
â”‚  - å¥åº·æ£€æŸ¥                                                       â”‚
â”‚  - æµé‡åˆ†å‘                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         API Gateway å±‚ (FastAPI) - Kubernetes Service            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚ API Pod 1  â”‚ API Pod 2  â”‚ API Pod 3  â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚  åŠŸèƒ½:                                                            â”‚
â”‚  - æ¥æ”¶ç”¨æˆ·è¯·æ±‚ (REST API)                                       â”‚
â”‚  - è¯·æ±‚éªŒè¯ã€é‰´æƒã€é™æµ                                          â”‚
â”‚  - ä»»åŠ¡åˆ›å»ºå¹¶æ¨é€åˆ°é˜Ÿåˆ—                                          â”‚
â”‚  - çŠ¶æ€æŸ¥è¯¢ API                                                  â”‚
â”‚  - Metrics æš´éœ²                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ä»»åŠ¡é˜Ÿåˆ—å±‚ (Redis Cluster)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚ é˜Ÿåˆ—åˆ†ç±» (æŒ‰ä»»åŠ¡ç±»å‹ + ä¼˜å…ˆçº§):              â”‚                â”‚
â”‚  â”‚  - queue:ti2v-5B:priority_1 (é«˜ä¼˜å…ˆçº§)       â”‚                â”‚
â”‚  â”‚  - queue:ti2v-5B:priority_0 (æ™®é€š)           â”‚                â”‚
â”‚  â”‚  - queue:t2v-A14B:priority_1                 â”‚                â”‚
â”‚  â”‚  - queue:t2v-A14B:priority_0                 â”‚                â”‚
â”‚  â”‚  - queue:i2v-A14B:priority_1                 â”‚                â”‚
â”‚  â”‚  - queue:i2v-A14B:priority_0                 â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚  ä»»åŠ¡çŠ¶æ€å­˜å‚¨: task:{task_id} â†’ JSON                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                GPU Worker è®¡ç®—å±‚ (Kubernetes Pods)               â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TI2V-5B Worker Pool â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Worker 1    Worker 2    Worker 3    ...    Worker N      â”‚  â”‚
â”‚  â”‚  (A10G 24GB) (A10G 24GB) (A10G 24GB)        (A10G 24GB)   â”‚  â”‚
â”‚  â”‚  g5.xlarge   g5.xlarge   g5.xlarge          g5.xlarge     â”‚  â”‚
â”‚  â”‚  å¤„ç†: 480P/720P TI2V ä»»åŠ¡                                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ T2V/I2V-A14B Worker Pool â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Worker 1          Worker 2          ...                  â”‚  â”‚
â”‚  â”‚  (A100 40GB)       (A100 40GB)                            â”‚  â”‚
â”‚  â”‚  p4d.24xlarge      p4d.24xlarge                           â”‚  â”‚
â”‚  â”‚  å¤„ç†: 720P A14B é«˜è´¨é‡ä»»åŠ¡                               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                   â”‚
â”‚  æ¯ä¸ª Worker:                                                     â”‚
â”‚  - ä» Redis è·å–ä»»åŠ¡ (BRPOP é˜»å¡å¼)                              â”‚
â”‚  - åŠ è½½æ¨¡å‹ (å¸¦ç¼“å­˜)                                             â”‚
â”‚  - æ‰§è¡Œè§†é¢‘ç”Ÿæˆ                                                  â”‚
â”‚  - æ›´æ–°ä»»åŠ¡è¿›åº¦åˆ° Redis                                          â”‚
â”‚  - ä¸Šä¼ ç»“æœåˆ° S3                                                 â”‚
â”‚  - è¿”å›ä»»åŠ¡å®ŒæˆçŠ¶æ€                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      å…±äº«å­˜å‚¨å±‚                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ EFS (æ¨¡å‹å­˜å‚¨)   â”‚ S3 (è§†é¢‘è¾“å‡º)                        â”‚    â”‚
â”‚  â”‚ - 1TB+ å®¹é‡      â”‚ - æ— é™æ‰©å±•                           â”‚    â”‚
â”‚  â”‚ - ReadOnlyMany   â”‚ - 7å¤©é¢„ç­¾åURL                       â”‚    â”‚
â”‚  â”‚ - æ‰€æœ‰Workerå…±äº« â”‚ - ç”Ÿå‘½å‘¨æœŸç®¡ç†                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 ç›‘æ§ä¸æ—¥å¿—å±‚                                      â”‚
â”‚  - Prometheus: æŒ‡æ ‡æ”¶é›† (GPUåˆ©ç”¨ç‡ã€é˜Ÿåˆ—é•¿åº¦ã€å»¶è¿Ÿç­‰)            â”‚
â”‚  - Grafana: å¯è§†åŒ–ä»ªè¡¨ç›˜                                         â”‚
â”‚  - CloudWatch/ELK: æ—¥å¿—èšåˆ                                      â”‚
â”‚  - AlertManager: å‘Šè­¦é€šçŸ¥                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 æ•°æ®æµ

```
ç”¨æˆ·è¯·æ±‚æµç¨‹:
1. User â†’ ALB â†’ API Server
2. API Server â†’ åˆ›å»ºä»»åŠ¡ â†’ Redis (task:{id} + æ¨é€åˆ°é˜Ÿåˆ—)
3. API Server â†’ è¿”å› task_id
4. User â†’ è½®è¯¢ /status/{task_id}

Worker å¤„ç†æµç¨‹:
1. Worker â†’ Redis BRPOP (é˜»å¡ç­‰å¾…ä»»åŠ¡)
2. Worker â†’ åŠ è½½æ¨¡å‹ (from EFS)
3. Worker â†’ ç”Ÿæˆè§†é¢‘ (GPUè®¡ç®—)
4. Worker â†’ æ›´æ–°è¿›åº¦ (Redis)
5. Worker â†’ ä¸Šä¼  S3
6. Worker â†’ æ›´æ–°çŠ¶æ€ä¸º completed (Redis)
7. Worker â†’ å¾ªç¯å›åˆ°æ­¥éª¤1

ç”¨æˆ·æŸ¥è¯¢æµç¨‹:
1. User â†’ /status/{task_id}
2. API Server â†’ Redis GET task:{id}
3. API Server â†’ è¿”å›çŠ¶æ€ + S3 URL (å¦‚æœå·²å®Œæˆ)
```

### 2.3 æ ¸å¿ƒç»„ä»¶è¯´æ˜

| ç»„ä»¶ | æŠ€æœ¯ | å‰¯æœ¬æ•° | èµ„æº | è¯´æ˜ |
|------|------|--------|------|------|
| **ALB** | AWS ALB | 1 | - | 7å±‚è´Ÿè½½å‡è¡¡ï¼ŒSSLç»ˆæ­¢ |
| **API Server** | FastAPI | 2-5 | 2GB RAM, 1 CPU | æ— çŠ¶æ€ï¼Œå¯æ°´å¹³æ‰©å±• |
| **Redis** | Redis 7 | 1-3 | 4GB RAM, 2 CPU | ä»»åŠ¡é˜Ÿåˆ— + çŠ¶æ€å­˜å‚¨ |
| **GPU Worker (TI2V)** | Python + PyTorch | 1-20 | 24GB VRAM, 32GB RAM | A10G GPU |
| **GPU Worker (A14B)** | Python + PyTorch | 0-10 | 40GB VRAM, 64GB RAM | A100 GPU |
| **EFS** | AWS EFS | 1 | 1TB+ | æ¨¡å‹æ–‡ä»¶å…±äº« |
| **S3** | AWS S3 | 1 | æ— é™ | è§†é¢‘è¾“å‡ºå­˜å‚¨ |

---

## 3. æŠ€æœ¯æ ˆé€‰å‹

### 3.1 å®Œæ•´æŠ€æœ¯æ ˆ

```yaml
åŸºç¡€è®¾æ–½å±‚:
  äº‘å¹³å°: AWS (EKS + EC2 + EFS + S3)
  å®¹å™¨ç¼–æ’: Kubernetes 1.28+
  å®¹å™¨è¿è¡Œæ—¶: containerd
  ç½‘ç»œ: AWS VPC + CNI

GPU ç®¡ç†å±‚:
  GPU è°ƒåº¦: NVIDIA GPU Operator
  GPU é©±åŠ¨: NVIDIA Driver 535+
  CUDA è¿è¡Œæ—¶: CUDA 12.2

åº”ç”¨å±‚:
  API ç½‘å…³: FastAPI 0.104+
  ä»»åŠ¡é˜Ÿåˆ—: Redis 7.0+
  Worker è¿è¡Œæ—¶: Python 3.10 + PyTorch 2.4+

å­˜å‚¨å±‚:
  å…±äº«å­˜å‚¨: AWS EFS (NFS)
  å¯¹è±¡å­˜å‚¨: AWS S3

ç›‘æ§å±‚:
  æŒ‡æ ‡æ”¶é›†: Prometheus
  å¯è§†åŒ–: Grafana
  æ—¥å¿—: CloudWatch Logs / ELK
  å‘Šè­¦: AlertManager + SNS

CI/CD:
  ä»£ç ä»“åº“: GitHub
  å®¹å™¨é•œåƒ: AWS ECR
  è‡ªåŠ¨éƒ¨ç½²: GitHub Actions

å¼€å‘å·¥å…·:
  IaC: Terraform / eksctl
  åŒ…ç®¡ç†: Helm
  é…ç½®ç®¡ç†: Kubernetes ConfigMap/Secrets
```

### 3.2 æŠ€æœ¯é€‰å‹ç†ç”±

| æŠ€æœ¯ | ä¸ºä»€ä¹ˆé€‰æ‹© | æ›¿ä»£æ–¹æ¡ˆ |
|------|------------|----------|
| **Kubernetes** | æˆç†Ÿçš„å®¹å™¨ç¼–æ’ï¼ŒGPUæ”¯æŒå®Œå–„ï¼Œç”Ÿæ€ä¸°å¯Œ | Docker Swarm (åŠŸèƒ½å¼±), Nomad (ç”Ÿæ€å°) |
| **Redis** | é«˜æ€§èƒ½é˜Ÿåˆ—ï¼Œæ”¯æŒé˜»å¡å¼è·å–ï¼Œç®€å•å¯é  | RabbitMQ (å¤æ‚), Kafka (è¿‡é‡) |
| **FastAPI** | é«˜æ€§èƒ½å¼‚æ­¥æ¡†æ¶ï¼Œè‡ªåŠ¨APIæ–‡æ¡£ï¼Œç±»å‹å®‰å…¨ | Flask (åŒæ­¥), Django (è¿‡é‡) |
| **EFS** | åŸç”Ÿæ”¯æŒNFSï¼Œå¤šAZé«˜å¯ç”¨ï¼Œå¼¹æ€§æ‰©å±• | FSx Lustre (è´µ), EBS (å•AZ) |
| **S3** | æ— é™å­˜å‚¨ï¼Œé«˜å¯ç”¨ï¼Œç”Ÿå‘½å‘¨æœŸç®¡ç† | EBS (å®¹é‡é™åˆ¶), è‡ªå»ºå­˜å‚¨ (ç»´æŠ¤æˆæœ¬) |
| **Prometheus** | K8s ç”Ÿæ€æ ‡å‡†ï¼Œä¸°å¯Œçš„æŒ‡æ ‡ï¼Œå¼ºå¤§çš„æŸ¥è¯¢ | CloudWatch (è´µ), Datadog (è´µ) |

---

## 4. å¼€æºGPUé›†ç¾¤ç®¡ç†æ–¹æ¡ˆå¯¹æ¯”

### 4.1 ä¸»æµæ–¹æ¡ˆå¯¹æ¯”

| æ–¹æ¡ˆ | ä¼˜åŠ¿ | åŠ£åŠ¿ | å­¦ä¹ æ›²çº¿ | ç”Ÿæ€æˆç†Ÿåº¦ | æ¨èåº¦ |
|------|------|------|----------|-----------|--------|
| **Kubernetes + GPU Operator** | â€¢ æˆç†Ÿç¨³å®š<br>â€¢ ç”Ÿæ€ä¸°å¯Œ<br>â€¢ è‡ªåŠ¨æ‰©ç¼©å®¹<br>â€¢ æ•…éšœè‡ªæ„ˆ<br>â€¢ æ··åˆäº‘æ”¯æŒ | â€¢ é…ç½®å¤æ‚<br>â€¢ å­¦ä¹ æˆæœ¬é«˜ | é™¡å³­ | â­â­â­â­â­ | â­â­â­â­â­ |
| **Ray Cluster** | â€¢ AI/ML ä¸“ç”¨<br>â€¢ Python APIç®€å•<br>â€¢ è‡ªåŠ¨æ‰¹å¤„ç†<br>â€¢ åŠ¨æ€èµ„æºåˆ†é… | â€¢ ç¤¾åŒºè¾ƒå°<br>â€¢ è°ƒè¯•å›°éš¾ | ä¸­ç­‰ | â­â­â­â­ | â­â­â­â­ |
| **Slurm** | â€¢ HPC æ ‡å‡†<br>â€¢ å…¬å¹³è°ƒåº¦<br>â€¢ è¯¦ç»†ç»Ÿè®¡ | â€¢ é…ç½®æå¤æ‚<br>â€¢ ä¸é€‚åˆäº‘<br>â€¢ æ— å®¹å™¨æ”¯æŒ | é™¡å³­ | â­â­â­â­ | â­â­â­ |
| **Nomad** | â€¢ è½»é‡çº§<br>â€¢ é…ç½®ç®€å• | â€¢ ç”Ÿæ€å¼±<br>â€¢ GPUæ”¯æŒæœ‰é™ | å¹³ç¼“ | â­â­â­ | â­â­â­ |
| **Docker Swarm** | â€¢ æç®€<br>â€¢ æ˜“ä¸Šæ‰‹ | â€¢ åŠŸèƒ½æœ‰é™<br>â€¢ ç¤¾åŒºèç¼© | å¹³ç¼“ | â­â­ | â­â­ |
| **è‡ªç ”è°ƒåº¦å™¨** | â€¢ å®Œå…¨å®šåˆ¶ | â€¢ å¼€å‘æˆæœ¬é«˜<br>â€¢ ç»´æŠ¤å›°éš¾ | éå¸¸é™¡å³­ | - | â­ |

### 4.2 è¯¦ç»†åˆ†æ

#### æ–¹æ¡ˆ1: Kubernetes + NVIDIA GPU Operator â­â­â­â­â­ (æ¨è)

**æ¶æ„:**
```yaml
Kubernetes Cluster
â”œâ”€â”€ NVIDIA GPU Operator
â”‚   â”œâ”€â”€ GPU Device Plugin (GPUèµ„æºå‘ç°å’Œåˆ†é…)
â”‚   â”œâ”€â”€ GPU Feature Discovery (GPUç‰¹æ€§æ ‡ç­¾)
â”‚   â”œâ”€â”€ DCGM Exporter (GPUæŒ‡æ ‡å¯¼å‡º)
â”‚   â””â”€â”€ MIG Manager (å¤šå®ä¾‹GPUæ”¯æŒ)
â”œâ”€â”€ Scheduler (è°ƒåº¦å™¨)
â”œâ”€â”€ Node Autoscaler (èŠ‚ç‚¹è‡ªåŠ¨æ‰©ç¼©å®¹)
â””â”€â”€ HPA (Podè‡ªåŠ¨æ‰©ç¼©å®¹)
```

**ä¼˜åŠ¿:**
- âœ… **ç”Ÿæ€æˆç†Ÿ**: CNCF æ¯•ä¸šé¡¹ç›®ï¼Œå¤§é‡æˆåŠŸæ¡ˆä¾‹
- âœ… **GPU æ”¯æŒå®Œå–„**: åŸç”Ÿæ”¯æŒ GPU è°ƒåº¦ã€MIGã€Time-Slicing
- âœ… **è‡ªåŠ¨æ‰©ç¼©å®¹**: æ ¹æ®è´Ÿè½½è‡ªåŠ¨å¢å‡ GPU èŠ‚ç‚¹
- âœ… **æ•…éšœè‡ªæ„ˆ**: Pod è‡ªåŠ¨é‡å¯ï¼ŒèŠ‚ç‚¹æ•…éšœè‡ªåŠ¨è¿ç§»
- âœ… **ç›‘æ§å®Œå–„**: Prometheus + Grafana å¼€ç®±å³ç”¨
- âœ… **äº‘åŸç”Ÿ**: æ”¯æŒ AWSã€GCPã€Azure ç­‰æ‰€æœ‰ä¸»æµäº‘å¹³å°

**é€‚ç”¨åœºæ™¯:**
- âœ… ç”Ÿäº§ç¯å¢ƒå¤§è§„æ¨¡éƒ¨ç½²
- âœ… éœ€è¦é«˜å¯ç”¨å’Œå¼¹æ€§æ‰©å±•
- âœ… å›¢é˜Ÿç†Ÿæ‚‰å®¹å™¨æŠ€æœ¯

**å¿«é€Ÿå¼€å§‹:**
```bash
# 1. åˆ›å»º EKS é›†ç¾¤
eksctl create cluster --name gpu-cluster --node-type g5.xlarge --nodes 2

# 2. å®‰è£… GPU Operator
helm install gpu-operator nvidia/gpu-operator -n gpu-operator --create-namespace

# 3. éƒ¨ç½²åº”ç”¨
kubectl apply -f deployment.yaml
```

---

#### æ–¹æ¡ˆ2: Ray Cluster â­â­â­â­

**æ¶æ„:**
```python
Ray Cluster
â”œâ”€â”€ Ray Head (é›†ç¾¤ç®¡ç†)
â”œâ”€â”€ Ray Workers (GPUæ‰§è¡ŒèŠ‚ç‚¹)
â”‚   â”œâ”€â”€ Worker 1 (1 GPU)
â”‚   â”œâ”€â”€ Worker 2 (1 GPU)
â”‚   â””â”€â”€ Worker N (1 GPU)
â”œâ”€â”€ Ray Serve (æ¨¡å‹æœåŠ¡)
â””â”€â”€ Ray Dashboard (ç›‘æ§é¢æ¿)
```

**ç¤ºä¾‹ä»£ç :**
```python
import ray
from ray import serve

# åˆå§‹åŒ– Ray
ray.init(address="auto")

@serve.deployment(
    num_replicas=3,
    ray_actor_options={"num_gpus": 1}
)
class VideoGenerator:
    def __init__(self):
        from wan.textimage2video import WanTI2V
        self.model = WanTI2V(ckpt_dir="/models/TI2V-5B")

    async def __call__(self, request):
        prompt = await request.json()
        video = await self.model.generate(prompt["prompt"])
        return {"video_url": upload_to_s3(video)}

# éƒ¨ç½²
serve.run(VideoGenerator.bind(), route_prefix="/generate")
```

**ä¼˜åŠ¿:**
- âœ… **Python åŸç”Ÿ**: çº¯ Python APIï¼Œæ— éœ€å­¦ä¹  YAML
- âœ… **è‡ªåŠ¨æ‰¹å¤„ç†**: æ™ºèƒ½åˆå¹¶å¤šä¸ªè¯·æ±‚ï¼Œæé«˜ GPU åˆ©ç”¨ç‡
- âœ… **åŠ¨æ€èµ„æºåˆ†é…**: æ ¹æ®è´Ÿè½½è‡ªåŠ¨è°ƒæ•´èµ„æº
- âœ… **åˆ†å¸ƒå¼è®¡ç®—**: æ”¯æŒæ•°æ®å¹¶è¡Œã€æ¨¡å‹å¹¶è¡Œ

**åŠ£åŠ¿:**
- âŒ ç¤¾åŒºç›¸å¯¹è¾ƒå°
- âŒ è°ƒè¯•å›°éš¾ï¼ˆåˆ†å¸ƒå¼ç¯å¢ƒï¼‰
- âŒ æ–‡æ¡£ä¸å¦‚ K8s å®Œå–„

**é€‚ç”¨åœºæ™¯:**
- âœ… AI/ML å·¥ä½œè´Ÿè½½
- âœ… å›¢é˜Ÿä»¥ Python å¼€å‘ä¸ºä¸»
- âœ… éœ€è¦ç®€å•çš„éƒ¨ç½²æ–¹å¼

---

#### æ–¹æ¡ˆ3: Slurm â­â­â­

**æ¶æ„:**
```
Slurm Cluster
â”œâ”€â”€ slurmctld (æ§åˆ¶èŠ‚ç‚¹)
â”œâ”€â”€ slurmdbd (æ•°æ®åº“)
â””â”€â”€ slurmd (è®¡ç®—èŠ‚ç‚¹)
    â”œâ”€â”€ GPU Node 1
    â”œâ”€â”€ GPU Node 2
    â””â”€â”€ GPU Node N
```

**ç¤ºä¾‹æäº¤ä»»åŠ¡:**
```bash
#!/bin/bash
#SBATCH --job-name=video-gen
#SBATCH --gres=gpu:1
#SBATCH --mem=32G
#SBATCH --time=00:30:00

python generate.py --prompt "A cat playing piano"
```

**ä¼˜åŠ¿:**
- âœ… HPC é¢†åŸŸæ ‡å‡†ï¼Œè¶…ç®—ä¸­å¿ƒå¹¿æ³›ä½¿ç”¨
- âœ… å…¬å¹³è°ƒåº¦ç®—æ³•æˆç†Ÿ
- âœ… è¯¦ç»†çš„èµ„æºä½¿ç”¨ç»Ÿè®¡

**åŠ£åŠ¿:**
- âŒ é…ç½®æå…¶å¤æ‚
- âŒ ä¸é€‚åˆäº‘ç¯å¢ƒï¼ˆä¸ºæœ¬åœ°é›†ç¾¤è®¾è®¡ï¼‰
- âŒ æ— å®¹å™¨æ”¯æŒï¼ˆéœ€è¦æ‰‹åŠ¨é›†æˆï¼‰
- âŒ å­¦ä¹ æ›²çº¿é™¡å³­

**é€‚ç”¨åœºæ™¯:**
- âœ… ä¼ ç»Ÿ HPC ç¯å¢ƒ
- âœ… ç§‘ç ”æœºæ„
- âŒ **ä¸æ¨èç”¨äºäº‘åŸç”Ÿåº”ç”¨**

---

### 4.3 æ–¹æ¡ˆé€‰æ‹©å»ºè®®

```
åœºæ™¯é€‰æ‹©å†³ç­–æ ‘:

ä½ çš„å›¢é˜Ÿç†Ÿæ‚‰ Kubernetes å—?
â”œâ”€ æ˜¯ â†’ é€‰æ‹© Kubernetes + GPU Operator â­â­â­â­â­
â””â”€ å¦
    â””â”€ ä½ çš„åº”ç”¨æ˜¯ AI/ML ä¸ºä¸»å—?
        â”œâ”€ æ˜¯ â†’ é€‰æ‹© Ray Cluster â­â­â­â­
        â””â”€ å¦
            â””â”€ ä½ éœ€è¦ç”Ÿäº§çº§å¯é æ€§å—?
                â”œâ”€ æ˜¯ â†’ æŠ•èµ„å­¦ä¹  Kubernetes â­â­â­â­â­
                â””â”€ å¦ â†’ Docker Swarm (å¿«é€ŸåŸå‹) â­â­
```

**æˆ‘ä»¬çš„æ¨è: Kubernetes + GPU Operator + Redis**

ç†ç”±:
1. âœ… ç”Ÿäº§å°±ç»ªï¼Œç»è¿‡å¤§è§„æ¨¡éªŒè¯
2. âœ… å®Œæ•´çš„ GPU ç®¡ç†èƒ½åŠ›
3. âœ… å¼ºå¤§çš„æ‰©å±•æ€§å’Œå¯é æ€§
4. âœ… ä¸°å¯Œçš„ç›‘æ§å’Œè¿ç»´å·¥å…·
5. âœ… äº‘å¹³å°åŸç”Ÿæ”¯æŒ

---

## 5. æ¨èæ–¹æ¡ˆï¼šKubernetes + Redis

### 5.1 ä¸ºä»€ä¹ˆé€‰æ‹©è¿™ä¸ªç»„åˆ?

| éœ€æ±‚ | Kubernetes å¦‚ä½•æ»¡è¶³ | Redis å¦‚ä½•æ»¡è¶³ |
|------|---------------------|----------------|
| **é«˜å¯ç”¨** | å¤šå‰¯æœ¬ã€è‡ªåŠ¨é‡å¯ã€è·¨AZéƒ¨ç½² | ä¸»ä»å¤åˆ¶ã€å“¨å…µæ¨¡å¼ |
| **å¼¹æ€§æ‰©å±•** | HPA + Cluster Autoscaler | é›†ç¾¤æ¨¡å¼åˆ†ç‰‡ |
| **GPU è°ƒåº¦** | GPU Operator + Device Plugin | - |
| **ä»»åŠ¡é˜Ÿåˆ—** | - | List æ•°æ®ç»“æ„ + BRPOP |
| **çŠ¶æ€ç®¡ç†** | ConfigMap/Secrets | Key-Value å­˜å‚¨ |
| **ç›‘æ§** | Prometheus é›†æˆ | MONITOR å‘½ä»¤ + Exporter |
| **æˆæœ¬ä¼˜åŒ–** | Spot å®ä¾‹ + è‡ªåŠ¨ç¼©å®¹ | å†…å­˜ä¼˜åŒ– |

### 5.2 æŠ€æœ¯æ ˆè¯¦è§£

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Kubernetes Control Plane             â”‚
â”‚  - API Server: æ¥æ”¶æ‰€æœ‰è¯·æ±‚                     â”‚
â”‚  - Scheduler: GPU èµ„æºè°ƒåº¦                      â”‚
â”‚  - Controller Manager: å‰¯æœ¬æ§åˆ¶ã€å¥åº·æ£€æŸ¥       â”‚
â”‚  - etcd: é›†ç¾¤çŠ¶æ€å­˜å‚¨                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               GPU Node Group                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ NVIDIA GPU Operator                      â”‚   â”‚
â”‚  â”‚  - Device Plugin: æš´éœ² GPU èµ„æº          â”‚   â”‚
â”‚  â”‚  - Feature Discovery: GPU ç‰¹æ€§æ ‡ç­¾       â”‚   â”‚
â”‚  â”‚  - DCGM Exporter: GPU æŒ‡æ ‡é‡‡é›†           â”‚   â”‚
â”‚  â”‚  - MIG Manager: å¤šå®ä¾‹ GPU æ”¯æŒ          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                  â”‚
â”‚  èŠ‚ç‚¹é…ç½®:                                       â”‚
â”‚  - AMI: AWS Deep Learning AMI                   â”‚
â”‚  - Instance: g5.xlarge / p4d.24xlarge           â”‚
â”‚  - Labels: gpu-type=a10g / gpu-type=a100        â”‚
â”‚  - Taints: nvidia.com/gpu=true:NoSchedule       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Application Workloads                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ API Server Deployment                    â”‚   â”‚
â”‚  â”‚  - Replicas: 2-5                         â”‚   â”‚
â”‚  â”‚  - Resources: CPU 1, Memory 2Gi          â”‚   â”‚
â”‚  â”‚  - Autoscaling: CPU > 70%                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ GPU Worker Deployment                    â”‚   â”‚
â”‚  â”‚  - Replicas: 1-20 (autoscaling)          â”‚   â”‚
â”‚  â”‚  - Resources: GPU 1, CPU 8, Memory 32Gi  â”‚   â”‚
â”‚  â”‚  - Volume: EFS (æ¨¡å‹)                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Redis StatefulSet                        â”‚   â”‚
â”‚  â”‚  - Replicas: 3 (ä¸»ä»å¤åˆ¶)                â”‚   â”‚
â”‚  â”‚  - Resources: CPU 2, Memory 4Gi          â”‚   â”‚
â”‚  â”‚  - Persistence: EBS                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.3 å…³é”®ç‰¹æ€§

#### 5.3.1 GPU èµ„æºè°ƒåº¦

**Kubernetes åŸç”Ÿæ”¯æŒ:**
```yaml
apiVersion: v1
kind: Pod
spec:
  containers:
  - name: gpu-worker
    resources:
      limits:
        nvidia.com/gpu: 1  # è¯·æ±‚ 1 ä¸ª GPU
```

**GPU èŠ‚ç‚¹é€‰æ‹©:**
```yaml
nodeSelector:
  gpu-type: a10g  # åªè°ƒåº¦åˆ° A10G èŠ‚ç‚¹

tolerations:
- key: nvidia.com/gpu
  operator: Exists
  effect: NoSchedule
```

**GPU Time-Slicing (å…±äº«):**
```yaml
# å…è®¸å¤šä¸ª Pod å…±äº«åŒä¸€ä¸ª GPU
apiVersion: v1
kind: ConfigMap
metadata:
  name: gpu-sharing-config
data:
  any: |-
    version: v1
    sharing:
      timeSlicing:
        resources:
        - name: nvidia.com/gpu
          replicas: 4  # 1ä¸ªGPUè™šæ‹Ÿæˆ4ä¸ª
```

#### 5.3.2 ä»»åŠ¡é˜Ÿåˆ—è®¾è®¡

**Redis é˜Ÿåˆ—ç»“æ„:**
```
é˜Ÿåˆ—å‘½å: queue:{task_type}:priority_{level}

ç¤ºä¾‹:
- queue:ti2v-5B:priority_1    # é«˜ä¼˜å…ˆçº§ TI2V-5B ä»»åŠ¡
- queue:ti2v-5B:priority_0    # æ™®é€šä¼˜å…ˆçº§ TI2V-5B ä»»åŠ¡
- queue:t2v-A14B:priority_1   # é«˜ä¼˜å…ˆçº§ T2V-A14B ä»»åŠ¡
- queue:t2v-A14B:priority_0   # æ™®é€šä¼˜å…ˆçº§ T2V-A14B ä»»åŠ¡

æ•°æ®ç»“æ„: Redis List
- LPUSH: ä»»åŠ¡å…¥é˜Ÿ (ä»å·¦ä¾§æ¨å…¥)
- BRPOP: ä»»åŠ¡å‡ºé˜Ÿ (ä»å³ä¾§é˜»å¡å¼¹å‡ºï¼Œè¶…æ—¶5ç§’)

ä¼˜ç‚¹:
âœ… å…ˆè¿›å…ˆå‡º (FIFO)
âœ… é˜»å¡å¼è·å– (èŠ‚çœ CPU)
âœ… åŸå­æ“ä½œ (çº¿ç¨‹å®‰å…¨)
âœ… æ”¯æŒè¶…æ—¶ (é¿å…æ­»é”)
```

**ä»»åŠ¡çŠ¶æ€ç®¡ç†:**
```
Key: task:{task_id}
Value: JSON å¯¹è±¡

{
  "task_id": "abc-123-def",
  "prompt": "A cat playing piano",
  "task_type": "ti2v-5B",
  "resolution": "1280*704",
  "status": "processing",  # queued, processing, completed, failed
  "progress": 45,          # 0-100
  "result_url": null,      # S3 URL (å®Œæˆå)
  "error": null,           # é”™è¯¯ä¿¡æ¯ (å¤±è´¥æ—¶)
  "created_at": "2025-11-03T10:00:00Z",
  "updated_at": "2025-11-03T10:02:30Z"
}

TTL: 86400 ç§’ (24å°æ—¶è‡ªåŠ¨è¿‡æœŸ)
```

#### 5.3.3 è‡ªåŠ¨æ‰©ç¼©å®¹

**Pod è‡ªåŠ¨æ‰©ç¼©å®¹ (HPA):**
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: gpu-worker-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: gpu-worker
  minReplicas: 2
  maxReplicas: 20
  metrics:
  # åŸºäºé˜Ÿåˆ—é•¿åº¦æ‰©ç¼©å®¹
  - type: External
    external:
      metric:
        name: redis_queue_length
      target:
        type: AverageValue
        averageValue: "5"  # æ¯ä¸ª Worker å¤„ç† 5 ä¸ªä»»åŠ¡

  # åŸºäº GPU åˆ©ç”¨ç‡æ‰©ç¼©å®¹
  - type: Pods
    pods:
      metric:
        name: gpu_utilization_percent
      target:
        type: AverageValue
        averageValue: "80"  # GPU åˆ©ç”¨ç‡ > 80% æ—¶æ‰©å®¹
```

**èŠ‚ç‚¹è‡ªåŠ¨æ‰©ç¼©å®¹ (Cluster Autoscaler):**
```bash
# è‡ªåŠ¨æ ¹æ® Pending Pods å¢åŠ  GPU èŠ‚ç‚¹
# è‡ªåŠ¨åˆ é™¤ç©ºé—²èŠ‚ç‚¹ (èŠ‚çœæˆæœ¬)

eksctl create nodegroup \
  --cluster gpu-cluster \
  --name gpu-workers \
  --node-type g5.xlarge \
  --nodes-min 1 \
  --nodes-max 50 \
  --asg-access  # å¯ç”¨ Autoscaler
```

---

## 6. å®Œæ•´éƒ¨ç½²æ­¥éª¤

### 6.1 å‰ç½®å‡†å¤‡

**æ‰€éœ€å·¥å…·:**
```bash
# å®‰è£… kubectl
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo install kubectl /usr/local/bin/

# å®‰è£… eksctl
curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
sudo mv /tmp/eksctl /usr/local/bin

# å®‰è£… Helm
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

# å®‰è£… AWS CLI
pip install awscli
aws configure
```

**AWS è´¦å·æƒé™è¦æ±‚:**
- âœ… EC2 (åˆ›å»º GPU å®ä¾‹)
- âœ… EKS (åˆ›å»º Kubernetes é›†ç¾¤)
- âœ… EFS (æ–‡ä»¶ç³»ç»Ÿ)
- âœ… S3 (å¯¹è±¡å­˜å‚¨)
- âœ… ECR (å®¹å™¨é•œåƒä»“åº“)
- âœ… IAM (è§’è‰²å’Œç­–ç•¥)
- âœ… VPC (ç½‘ç»œé…ç½®)

### 6.2 Step 1: åˆ›å»º EKS é›†ç¾¤

**åˆ›å»ºé›†ç¾¤é…ç½®æ–‡ä»¶:**
```yaml
# eks-cluster-config.yaml
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: wan22-cluster
  region: us-east-2
  version: "1.28"

# IAM é…ç½®
iam:
  withOIDC: true  # å¯ç”¨ IRSA (IAM Roles for Service Accounts)

# VPC é…ç½®
vpc:
  cidr: 10.0.0.0/16
  nat:
    gateway: Single  # å• NAT ç½‘å…³ (èŠ‚çœæˆæœ¬)

# æ§åˆ¶å¹³é¢èŠ‚ç‚¹
managedNodeGroups:
  # æ™®é€šèŠ‚ç‚¹ç»„ (API Server, Redis ç­‰)
  - name: general-nodes
    instanceType: m5.large
    minSize: 2
    maxSize: 5
    desiredCapacity: 2
    volumeSize: 50
    ssh:
      allow: false
    labels:
      role: general
    tags:
      k8s.io/cluster-autoscaler/enabled: "true"
      k8s.io/cluster-autoscaler/wan22-cluster: "owned"

  # GPU èŠ‚ç‚¹ç»„ - A10G (TI2V-5B)
  - name: gpu-a10g-nodes
    instanceType: g5.xlarge
    minSize: 1
    maxSize: 20
    desiredCapacity: 2
    volumeSize: 100
    ssh:
      allow: false
    labels:
      role: gpu-worker
      gpu-type: a10g
    taints:
    - key: nvidia.com/gpu
      value: "true"
      effect: NoSchedule
    tags:
      k8s.io/cluster-autoscaler/enabled: "true"
      k8s.io/cluster-autoscaler/wan22-cluster: "owned"
      k8s.io/cluster-autoscaler/node-template/label/gpu-type: a10g
    # ä½¿ç”¨ Spot å®ä¾‹ (èŠ‚çœ 70% æˆæœ¬)
    spot: true

  # GPU èŠ‚ç‚¹ç»„ - A100 (T2V/I2V-A14B)
  - name: gpu-a100-nodes
    instanceType: p4d.24xlarge
    minSize: 0
    maxSize: 10
    desiredCapacity: 1
    volumeSize: 200
    ssh:
      allow: false
    labels:
      role: gpu-worker
      gpu-type: a100
    taints:
    - key: nvidia.com/gpu
      value: "true"
      effect: NoSchedule
    tags:
      k8s.io/cluster-autoscaler/enabled: "true"
      k8s.io/cluster-autoscaler/wan22-cluster: "owned"
      k8s.io/cluster-autoscaler/node-template/label/gpu-type: a100
    # A100 å¤ªè´µï¼Œä¸ç”¨ Spot (ç¨³å®šæ€§ä¼˜å…ˆ)
```

**æ‰§è¡Œåˆ›å»º:**
```bash
# åˆ›å»ºé›†ç¾¤ (éœ€è¦ 15-20 åˆ†é’Ÿ)
eksctl create cluster -f eks-cluster-config.yaml

# éªŒè¯é›†ç¾¤
kubectl get nodes

# è¾“å‡ºç¤ºä¾‹:
# NAME                                          STATUS   ROLES    AGE   VERSION
# ip-10-0-1-100.us-east-2.compute.internal      Ready    <none>   5m    v1.28.0
# ip-10-0-2-200.us-east-2.compute.internal      Ready    <none>   5m    v1.28.0
# ip-10-0-3-50.us-east-2.compute.internal       Ready    <none>   3m    v1.28.0

# æ£€æŸ¥ GPU èŠ‚ç‚¹
kubectl get nodes -l gpu-type=a10g
```

### 6.3 Step 2: å®‰è£… NVIDIA GPU Operator

```bash
# æ·»åŠ  NVIDIA Helm ä»“åº“
helm repo add nvidia https://helm.ngc.nvidia.com/nvidia
helm repo update

# å®‰è£… GPU Operator
# æ³¨æ„: AWS GPU AMI å·²åŒ…å«é©±åŠ¨ï¼Œæ‰€ä»¥ driver.enabled=false
helm install gpu-operator nvidia/gpu-operator \
  -n gpu-operator --create-namespace \
  --set driver.enabled=false \
  --set toolkit.enabled=true \
  --set devicePlugin.enabled=true \
  --set migManager.enabled=true \
  --set dcgmExporter.enabled=true

# ç­‰å¾…æ‰€æœ‰ Pod è¿è¡Œ
kubectl get pods -n gpu-operator -w

# éªŒè¯ GPU èµ„æº
kubectl describe node <gpu-node-name> | grep nvidia.com/gpu

# è¾“å‡ºç¤ºä¾‹:
#  nvidia.com/gpu:     1
#  nvidia.com/gpu:     1
```

### 6.4 Step 3: åˆ›å»º EFS æ–‡ä»¶ç³»ç»Ÿ (æ¨¡å‹å­˜å‚¨)

```bash
# åˆ›å»º EFS
EFS_ID=$(aws efs create-file-system \
  --region us-east-2 \
  --performance-mode generalPurpose \
  --throughput-mode bursting \
  --encrypted \
  --tags Key=Name,Value=wan22-models \
  --query 'FileSystemId' \
  --output text)

echo "EFS ID: $EFS_ID"

# ç­‰å¾… EFS å¯ç”¨
aws efs describe-file-systems --file-system-id $EFS_ID --region us-east-2

# è·å– VPC ID å’Œå­ç½‘ ID
VPC_ID=$(aws eks describe-cluster --name wan22-cluster --region us-east-2 --query 'cluster.resourcesVpcConfig.vpcId' --output text)
SUBNET_IDS=$(aws eks describe-cluster --name wan22-cluster --region us-east-2 --query 'cluster.resourcesVpcConfig.subnetIds' --output text)

# ä¸ºæ¯ä¸ªå­ç½‘åˆ›å»ºæŒ‚è½½ç›®æ ‡
for SUBNET_ID in $SUBNET_IDS; do
  aws efs create-mount-target \
    --file-system-id $EFS_ID \
    --subnet-id $SUBNET_ID \
    --region us-east-2
done

# å®‰è£… EFS CSI Driver
kubectl apply -k "github.com/kubernetes-sigs/aws-efs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-1.5"

# ç­‰å¾… CSI Driver è¿è¡Œ
kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-efs-csi-driver
```

### 6.5 Step 4: ä¸Šä¼ æ¨¡å‹åˆ° EFS

```bash
# åˆ›å»ºä¸´æ—¶ EC2 å®ä¾‹æŒ‚è½½ EFS
# æˆ–è€…åœ¨ç°æœ‰èŠ‚ç‚¹ä¸ŠæŒ‚è½½

# åœ¨ EKS èŠ‚ç‚¹ä¸Šæ‰§è¡Œ:
sudo mkdir -p /mnt/efs
sudo mount -t efs -o tls $EFS_ID:/ /mnt/efs

# ä¸‹è½½æ¨¡å‹
cd /mnt/efs
mkdir -p models

# ä¸‹è½½ TI2V-5B
huggingface-cli download Wan-AI/Wan2.2-TI2V-5B \
  --local-dir ./models/Wan2.2-TI2V-5B

# ä¸‹è½½ T2V-A14B
huggingface-cli download Wan-AI/Wan2.2-T2V-A14B \
  --local-dir ./models/Wan2.2-T2V-A14B

# ä¸‹è½½ I2V-A14B
huggingface-cli download Wan-AI/Wan2.2-I2V-A14B \
  --local-dir ./models/Wan2.2-I2V-A14B

# éªŒè¯æ–‡ä»¶
ls -lh /mnt/efs/models/
```

### 6.6 Step 5: æ„å»ºå¹¶æ¨é€ Docker é•œåƒ

```bash
# ç™»å½• ECR
aws ecr get-login-password --region us-east-2 | \
  docker login --username AWS --password-stdin <account-id>.dkr.ecr.us-east-2.amazonaws.com

# åˆ›å»º ECR ä»“åº“ (å¦‚æœä¸å­˜åœ¨)
aws ecr create-repository \
  --repository-name wan22-prod \
  --region us-east-2

# æ„å»ºé•œåƒ
docker build -t wan22:latest -f Dockerfile .

# æ‰“æ ‡ç­¾
docker tag wan22:latest <account-id>.dkr.ecr.us-east-2.amazonaws.com/wan22-prod:latest
docker tag wan22:latest <account-id>.dkr.ecr.us-east-2.amazonaws.com/wan22-prod:$(git rev-parse --short HEAD)

# æ¨é€
docker push <account-id>.dkr.ecr.us-east-2.amazonaws.com/wan22-prod:latest
docker push <account-id>.dkr.ecr.us-east-2.amazonaws.com/wan22-prod:$(git rev-parse --short HEAD)
```

### 6.7 Step 6: éƒ¨ç½²åº”ç”¨åˆ° Kubernetes

**æ›´æ–°é…ç½®æ–‡ä»¶:**
```bash
# æ›¿æ¢ k8s-deployment.yaml ä¸­çš„å ä½ç¬¦
sed -i "s/fs-xxxxx/$EFS_ID/g" cluster/k8s-deployment.yaml
sed -i "s|268032756104.dkr.ecr.us-east-2.amazonaws.com|<your-account-id>.dkr.ecr.us-east-2.amazonaws.com|g" cluster/k8s-deployment.yaml
```

**éƒ¨ç½²:**
```bash
# åˆ›å»ºå‘½åç©ºé—´
kubectl create namespace wan22

# éƒ¨ç½²æ‰€æœ‰ç»„ä»¶
kubectl apply -f cluster/k8s-deployment.yaml

# æŸ¥çœ‹éƒ¨ç½²çŠ¶æ€
kubectl get pods -n wan22 -w

# è¾“å‡ºç¤ºä¾‹:
# NAME                           READY   STATUS    RESTARTS   AGE
# redis-0                        1/1     Running   0          2m
# api-server-7f8d9c5b6d-abcde    1/1     Running   0          2m
# api-server-7f8d9c5b6d-fghij    1/1     Running   0          2m
# worker-ti2v-5b-5d7c8-klmno     1/1     Running   0          2m
# worker-ti2v-5b-5d7c8-pqrst     1/1     Running   0          2m
# worker-a14b-6f9e8-uvwxy        1/1     Running   0          2m
```

### 6.8 Step 7: é…ç½® LoadBalancer

```bash
# è·å– LoadBalancer åœ°å€
kubectl get svc api-server -n wan22

# è¾“å‡ºç¤ºä¾‹:
# NAME         TYPE           CLUSTER-IP      EXTERNAL-IP                                                              PORT(S)        AGE
# api-server   LoadBalancer   10.100.50.123   a1b2c3d4e5f6-123456789.us-east-2.elb.amazonaws.com   80:31234/TCP   5m

API_URL=$(kubectl get svc api-server -n wan22 -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')

echo "API URL: http://$API_URL"
```

### 6.9 Step 8: æµ‹è¯•éƒ¨ç½²

```bash
# å¥åº·æ£€æŸ¥
curl http://$API_URL/api/v1/health

# æäº¤æµ‹è¯•ä»»åŠ¡
curl -X POST http://$API_URL/api/v1/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "A cat playing piano in a jazz club",
    "resolution": "832*480",
    "task_type": "ti2v-5B",
    "priority": 0
  }'

# å“åº”:
# {
#   "task_id": "abc-123-def-456",
#   "status": "queued",
#   "message": "Task created successfully",
#   "status_url": "/api/v1/status/abc-123-def-456"
# }

# æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€
TASK_ID="abc-123-def-456"
curl http://$API_URL/api/v1/status/$TASK_ID
```

---

## 7. API ä½¿ç”¨ç¤ºä¾‹

### 7.1 API ç«¯ç‚¹

| ç«¯ç‚¹ | æ–¹æ³• | è¯´æ˜ |
|------|------|------|
| `/api/v1/generate` | POST | åˆ›å»ºè§†é¢‘ç”Ÿæˆä»»åŠ¡ |
| `/api/v1/status/{task_id}` | GET | æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€ |
| `/api/v1/health` | GET | å¥åº·æ£€æŸ¥ |
| `/api/v1/metrics` | GET | ç³»ç»ŸæŒ‡æ ‡ |

### 7.2 åˆ›å»ºä»»åŠ¡

**è¯·æ±‚:**
```bash
curl -X POST http://api.example.com/api/v1/generate \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -d '{
    "prompt": "A beautiful sunset over mountains with birds flying",
    "resolution": "1280*704",
    "task_type": "ti2v-5B",
    "image_url": "https://example.com/image.jpg",  # å¯é€‰ï¼Œç”¨äº I2V
    "priority": 0,  # 0=normal, 1=high
    "callback_url": "https://your-service.com/webhook"  # å¯é€‰
  }'
```

**å“åº”:**
```json
{
  "task_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "queued",
  "message": "Task created successfully",
  "status_url": "/api/v1/status/550e8400-e29b-41d4-a716-446655440000",
  "estimated_time": 180  # é¢„è®¡ç­‰å¾…æ—¶é—´(ç§’)
}
```

### 7.3 æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€

**è¯·æ±‚:**
```bash
curl http://api.example.com/api/v1/status/550e8400-e29b-41d4-a716-446655440000 \
  -H "Authorization: Bearer YOUR_API_KEY"
```

**å“åº” (å¤„ç†ä¸­):**
```json
{
  "task_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "processing",
  "progress": 65,
  "result_url": null,
  "error": null,
  "created_at": "2025-11-03T10:00:00Z",
  "updated_at": "2025-11-03T10:02:30Z",
  "estimated_completion": "2025-11-03T10:05:00Z"
}
```

**å“åº” (å®Œæˆ):**
```json
{
  "task_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "completed",
  "progress": 100,
  "result_url": "https://s3.amazonaws.com/bucket/wan-videos/550e8400.../video.mp4",
  "thumbnail_url": "https://s3.amazonaws.com/bucket/wan-videos/550e8400.../thumb.jpg",
  "duration": 5.0,  # è§†é¢‘æ—¶é•¿(ç§’)
  "resolution": "1280x704",
  "file_size": 15728640,  # æ–‡ä»¶å¤§å°(å­—èŠ‚)
  "error": null,
  "created_at": "2025-11-03T10:00:00Z",
  "updated_at": "2025-11-03T10:04:45Z",
  "processing_time": 285  # å®é™…å¤„ç†æ—¶é—´(ç§’)
}
```

### 7.4 Python SDK ç¤ºä¾‹

```python
import requests
import time

class Wan22Client:
    def __init__(self, api_url, api_key):
        self.api_url = api_url
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }

    def generate_video(self, prompt, resolution="1280*704", task_type="ti2v-5B"):
        """åˆ›å»ºè§†é¢‘ç”Ÿæˆä»»åŠ¡"""
        response = requests.post(
            f"{self.api_url}/api/v1/generate",
            headers=self.headers,
            json={
                "prompt": prompt,
                "resolution": resolution,
                "task_type": task_type
            }
        )
        return response.json()

    def get_status(self, task_id):
        """æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€"""
        response = requests.get(
            f"{self.api_url}/api/v1/status/{task_id}",
            headers=self.headers
        )
        return response.json()

    def wait_for_completion(self, task_id, poll_interval=5, timeout=600):
        """ç­‰å¾…ä»»åŠ¡å®Œæˆ"""
        start_time = time.time()

        while True:
            if time.time() - start_time > timeout:
                raise TimeoutError(f"Task {task_id} timeout after {timeout}s")

            status = self.get_status(task_id)

            if status["status"] == "completed":
                return status
            elif status["status"] == "failed":
                raise Exception(f"Task failed: {status.get('error')}")

            print(f"Progress: {status['progress']}%")
            time.sleep(poll_interval)

# ä½¿ç”¨ç¤ºä¾‹
client = Wan22Client(
    api_url="http://api.example.com",
    api_key="your-api-key"
)

# åˆ›å»ºä»»åŠ¡
result = client.generate_video(
    prompt="A cat playing piano in a jazz club",
    resolution="1280*704"
)

print(f"Task created: {result['task_id']}")

# ç­‰å¾…å®Œæˆ
final_status = client.wait_for_completion(result['task_id'])

print(f"Video URL: {final_status['result_url']}")
```

---

## 8. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 8.1 GPU åˆ©ç”¨ç‡ä¼˜åŒ–

#### 8.1.1 æ¨¡å‹é¢„åŠ è½½

**é—®é¢˜:** æ¯æ¬¡å¤„ç†ä»»åŠ¡éƒ½é‡æ–°åŠ è½½æ¨¡å‹ï¼Œæµªè´¹æ—¶é—´

**è§£å†³æ–¹æ¡ˆ:** Worker å¯åŠ¨æ—¶é¢„åŠ è½½æ‰€æœ‰æ¨¡å‹

```python
# gpu_worker.py
class GPUWorker:
    def __init__(self):
        self.models = {}

        # é¢„åŠ è½½å¸¸ç”¨æ¨¡å‹
        self.preload_models()

    def preload_models(self):
        """å¯åŠ¨æ—¶é¢„åŠ è½½æ‰€æœ‰æ¨¡å‹"""
        print("ğŸš€ Preloading models...")

        for task_type in ["ti2v-5B", "t2v-A14B", "i2v-A14B"]:
            try:
                self.load_model(task_type)
                print(f"âœ… Preloaded: {task_type}")
            except Exception as e:
                print(f"âš ï¸  Failed to preload {task_type}: {e}")

        print(f"ğŸ‰ Preloaded {len(self.models)} models")
```

**æ•ˆæœ:**
- âœ… é¦–æ¬¡ä»»åŠ¡æ— éœ€ç­‰å¾…æ¨¡å‹åŠ è½½
- âœ… ä»»åŠ¡é—´åˆ‡æ¢æ›´å¿«
- âŒ å¢åŠ åˆå§‹å†…å­˜å ç”¨

#### 8.1.2 åŠ¨æ€æ‰¹å¤„ç† (Batching)

**é—®é¢˜:** å•ä¸ªä»»åŠ¡æ— æ³•å……åˆ†åˆ©ç”¨ GPU

**è§£å†³æ–¹æ¡ˆ:** åˆå¹¶å¤šä¸ªä»»åŠ¡æ‰¹é‡å¤„ç†

```python
class GPUWorker:
    def process_batch(self, max_batch_size=4, wait_timeout=5):
        """æ‰¹é‡å¤„ç†ä»»åŠ¡"""
        batch = []

        # æ”¶é›†ä»»åŠ¡
        start_time = time.time()
        while len(batch) < max_batch_size:
            # å‰©ä½™æ—¶é—´
            remaining = wait_timeout - (time.time() - start_time)
            if remaining <= 0:
                break

            # ä»é˜Ÿåˆ—è·å–ä»»åŠ¡ (é˜»å¡)
            result = self.redis.brpop(self.queues, timeout=int(remaining))
            if result:
                queue_name, task_id = result
                batch.append(task_id)

        if not batch:
            return

        # æ‰¹é‡ç”Ÿæˆ
        prompts = [self.get_task_data(tid)['prompt'] for tid in batch]
        videos = self.model.generate_batch(prompts)  # å‡è®¾æ”¯æŒæ‰¹é‡ç”Ÿæˆ

        # æ‰¹é‡ä¸Šä¼ 
        for task_id, video in zip(batch, videos):
            self.upload_result(task_id, video)
```

**æ•ˆæœ:**
- âœ… GPU åˆ©ç”¨ç‡æå‡ 30-50%
- âœ… ååé‡æå‡ 2-3x
- âŒ å•ä¸ªä»»åŠ¡å»¶è¿Ÿå¢åŠ 

**é€‚ç”¨åœºæ™¯:** é«˜å¹¶å‘æ—¶æ®µ

#### 8.1.3 GPU MIG åˆ†ç‰‡ (Multi-Instance GPU)

**é—®é¢˜:** A100 40GB è¿‡äºå¼ºå¤§ï¼Œå°ä»»åŠ¡æµªè´¹èµ„æº

**è§£å†³æ–¹æ¡ˆ:** å°† 1 ä¸ª A100 åˆ†æˆå¤šä¸ªå°å®ä¾‹

```yaml
# MIG é…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: mig-config
  namespace: gpu-operator
data:
  config.yaml: |
    version: v1
    mig-configs:
      # å°† A100 40GB åˆ†æˆ 3 ä¸ªå®ä¾‹
      all-3g.20gb:
        - devices: [0]
          mig-devices:
            "3g.20gb": 3  # æ¯ä¸ª 20GB
```

**ä½¿ç”¨ MIG å®ä¾‹:**
```yaml
resources:
  limits:
    nvidia.com/mig-3g.20gb: 1  # è¯·æ±‚ 1 ä¸ª 20GB MIG å®ä¾‹
```

**æ•ˆæœ:**
- âœ… 1 ä¸ª A100 å¯åŒæ—¶å¤„ç† 3 ä¸ªä»»åŠ¡
- âœ… æˆæœ¬æ•ˆç‡æå‡ 3x
- âŒ å•ä»»åŠ¡æ€§èƒ½ç•¥æœ‰ä¸‹é™

### 8.2 å†…å­˜ä¼˜åŒ–

#### 8.2.1 æ¨¡å‹é‡åŒ– (Quantization)

```python
# ä½¿ç”¨ bfloat16 ä»£æ›¿ float32
model = WanTI2V(
    ckpt_dir="/models/TI2V-5B",
    convert_model_dtype=True  # è½¬æ¢ä¸º bf16
)

# èŠ‚çœå†…å­˜: ~50%
# æ€§èƒ½å½±å“: å‡ ä¹æ— 
```

#### 8.2.2 æ¸è¿›å¼åŠ è½½

```python
def generate_with_progressive_loading(self, prompt):
    """æ¸è¿›å¼åŠ è½½æ¨¡å‹ç»„ä»¶"""

    # 1. åŠ è½½ T5 ç¼–ç å™¨
    text_emb = self.t5.encode(prompt)

    # 2. é‡Šæ”¾ T5ï¼ŒåŠ è½½ DiT
    self.t5.cpu()
    torch.cuda.empty_cache()

    # 3. ç”Ÿæˆ latents
    latents = self.dit.generate(text_emb)

    # 4. é‡Šæ”¾ DiTï¼ŒåŠ è½½ VAE
    self.dit.cpu()
    torch.cuda.empty_cache()

    # 5. è§£ç è§†é¢‘
    video = self.vae.decode(latents)

    return video
```

### 8.3 ç½‘ç»œä¼˜åŒ–

#### 8.3.1 ä½¿ç”¨ VPC Endpoints

**é—®é¢˜:** è®¿é—® S3/ECR èµ°å…¬ç½‘ï¼Œæ…¢ä¸”äº§ç”Ÿè´¹ç”¨

**è§£å†³æ–¹æ¡ˆ:** é…ç½® VPC Endpoints

```bash
# åˆ›å»º S3 VPC Endpoint
aws ec2 create-vpc-endpoint \
  --vpc-id $VPC_ID \
  --service-name com.amazonaws.us-east-2.s3 \
  --route-table-ids $ROUTE_TABLE_ID

# åˆ›å»º ECR VPC Endpoint
aws ec2 create-vpc-endpoint \
  --vpc-id $VPC_ID \
  --service-name com.amazonaws.us-east-2.ecr.api \
  --vpc-endpoint-type Interface \
  --subnet-ids $SUBNET_ID
```

**æ•ˆæœ:**
- âœ… ä¸Šä¼ /ä¸‹è½½é€Ÿåº¦æå‡ 3-5x
- âœ… èŠ‚çœæ•°æ®ä¼ è¾“æˆæœ¬
- âœ… æé«˜å®‰å…¨æ€§

#### 8.3.2 æœ¬åœ°ç¼“å­˜çƒ­ç‚¹æ–‡ä»¶

```python
# Worker æœ¬åœ°ç¼“å­˜å¸¸ç”¨å›¾ç‰‡
from functools import lru_cache

@lru_cache(maxsize=100)
def download_image(url):
    """ä¸‹è½½å¹¶ç¼“å­˜å›¾ç‰‡"""
    response = requests.get(url)
    return Image.open(BytesIO(response.content))
```

### 8.4 è°ƒåº¦ä¼˜åŒ–

#### 8.4.1 ä»»åŠ¡ä¼˜å…ˆçº§é˜Ÿåˆ—

```python
# API Server
def create_task(request):
    # VIP ç”¨æˆ·ä½¿ç”¨é«˜ä¼˜å…ˆçº§é˜Ÿåˆ—
    if user.is_vip:
        queue = f"queue:{task_type}:priority_1"
    else:
        queue = f"queue:{task_type}:priority_0"

    redis.lpush(queue, task_id)
```

#### 8.4.2 GPU äº²å’Œæ€§è°ƒåº¦

```yaml
# å°†ç›¸åŒç±»å‹çš„ä»»åŠ¡è°ƒåº¦åˆ°åŒä¸€ GPU èŠ‚ç‚¹
# é¿å…æ¨¡å‹é¢‘ç¹åˆ‡æ¢
affinity:
  podAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: task-type
            operator: In
            values:
            - ti2v-5B
        topologyKey: kubernetes.io/hostname
```

---

## 9. ç›‘æ§ä¸å‘Šè­¦

### 9.1 å®‰è£… Prometheus + Grafana

```bash
# æ·»åŠ  Prometheus Helm ä»“åº“
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

# å®‰è£… kube-prometheus-stack (åŒ…å« Prometheus + Grafana + AlertManager)
helm install prometheus prometheus-community/kube-prometheus-stack \
  -n monitoring --create-namespace \
  --set prometheus.prometheusSpec.retention=30d \
  --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi \
  --set grafana.adminPassword=your-secure-password

# è®¿é—® Grafana
kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80

# æ‰“å¼€æµè§ˆå™¨: http://localhost:3000
# ç”¨æˆ·å: admin
# å¯†ç : your-secure-password
```

### 9.2 å…³é”®æŒ‡æ ‡

#### 9.2.1 GPU æŒ‡æ ‡

| æŒ‡æ ‡ | Prometheus Query | å‘Šè­¦é˜ˆå€¼ |
|------|------------------|----------|
| **GPU åˆ©ç”¨ç‡** | `DCGM_FI_DEV_GPU_UTIL` | < 60% (æµªè´¹), > 95% (è¿‡è½½) |
| **GPU å†…å­˜ä½¿ç”¨ç‡** | `DCGM_FI_DEV_FB_USED / DCGM_FI_DEV_FB_FREE * 100` | > 90% |
| **GPU æ¸©åº¦** | `DCGM_FI_DEV_GPU_TEMP` | > 85Â°C |
| **GPU åŠŸç‡** | `DCGM_FI_DEV_POWER_USAGE` | > 90% max power |

#### 9.2.2 åº”ç”¨æŒ‡æ ‡

| æŒ‡æ ‡ | è¯´æ˜ | å‘Šè­¦é˜ˆå€¼ |
|------|------|----------|
| **é˜Ÿåˆ—é•¿åº¦** | `redis_queue_length` | > 100 |
| **ä»»åŠ¡å¤±è´¥ç‡** | `task_failed / task_total * 100` | > 5% |
| **å¹³å‡å¤„ç†æ—¶é—´** | `avg(task_duration)` | > 360s (6åˆ†é’Ÿ) |
| **API å“åº”æ—¶é—´** | `http_request_duration_p95` | > 500ms |
| **Worker é‡å¯æ¬¡æ•°** | `kube_pod_restart_total` | > 3 /hour |

#### 9.2.3 ç³»ç»ŸæŒ‡æ ‡

| æŒ‡æ ‡ | è¯´æ˜ | å‘Šè­¦é˜ˆå€¼ |
|------|------|----------|
| **èŠ‚ç‚¹ CPU** | `node_cpu_usage` | > 80% |
| **èŠ‚ç‚¹å†…å­˜** | `node_memory_usage` | > 85% |
| **ç£ç›˜ IO** | `node_disk_io_time_seconds` | - |
| **ç½‘ç»œæµé‡** | `node_network_receive_bytes` | - |

### 9.3 Grafana ä»ªè¡¨ç›˜

**å¯¼å…¥ä»ªè¡¨ç›˜ID:**
- NVIDIA DCGM Exporter: `12239`
- Kubernetes Cluster Monitoring: `7249`
- Redis: `11835`

**è‡ªå®šä¹‰ä»ªè¡¨ç›˜ç¤ºä¾‹ (JSON):**
```json
{
  "dashboard": {
    "title": "Wan2.2 GPU Cluster Overview",
    "panels": [
      {
        "title": "GPU Utilization",
        "targets": [{
          "expr": "avg(DCGM_FI_DEV_GPU_UTIL) by (gpu, instance)"
        }],
        "type": "graph"
      },
      {
        "title": "Queue Length",
        "targets": [{
          "expr": "sum(redis_list_length{queue=~\"queue:.*\"})"
        }],
        "type": "stat"
      },
      {
        "title": "Task Status Distribution",
        "targets": [{
          "expr": "count(task_status) by (status)"
        }],
        "type": "piechart"
      }
    ]
  }
}
```

### 9.4 å‘Šè­¦è§„åˆ™

**Prometheus AlertManager é…ç½®:**
```yaml
# alert-rules.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-alert-rules
  namespace: monitoring
data:
  alert-rules.yaml: |
    groups:
    - name: gpu-alerts
      interval: 30s
      rules:
      # GPU åˆ©ç”¨ç‡è¿‡ä½
      - alert: GPUUtilizationLow
        expr: avg(DCGM_FI_DEV_GPU_UTIL) < 60
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "GPU utilization is low ({{ $value }}%)"
          description: "Consider scaling down GPU nodes to save cost"

      # GPU å†…å­˜ä¸è¶³
      - alert: GPUMemoryHigh
        expr: DCGM_FI_DEV_FB_USED / DCGM_FI_DEV_FB_FREE * 100 > 90
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "GPU memory usage is high ({{ $value }}%)"
          description: "GPU {{ $labels.gpu }} on {{ $labels.instance }} is running out of memory"

      # é˜Ÿåˆ—ç§¯å‹
      - alert: QueueBacklog
        expr: sum(redis_list_length{queue=~"queue:.*"}) > 100
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "Task queue has {{ $value }} pending tasks"
          description: "Consider scaling up GPU workers"

      # ä»»åŠ¡å¤±è´¥ç‡é«˜
      - alert: HighTaskFailureRate
        expr: rate(task_failed_total[5m]) / rate(task_total[5m]) > 0.05
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "Task failure rate is {{ $value | humanizePercentage }}"
          description: "Check worker logs for errors"

      # Worker é¢‘ç¹é‡å¯
      - alert: FrequentWorkerRestarts
        expr: rate(kube_pod_restart_total{namespace="wan22"}[1h]) > 3
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Pod {{ $labels.pod }} is restarting frequently"
          description: "Check for OOM or application crashes"
```

**é…ç½® AlertManager é€šçŸ¥ (Slack/Email/PagerDuty):**
```yaml
# alertmanager-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: monitoring
data:
  alertmanager.yml: |
    global:
      resolve_timeout: 5m

    route:
      group_by: ['alertname', 'cluster']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 12h
      receiver: 'slack'
      routes:
      - match:
          severity: critical
        receiver: 'pagerduty'

    receivers:
    - name: 'slack'
      slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'
        channel: '#alerts'
        title: '{{ .CommonAnnotations.summary }}'
        text: '{{ .CommonAnnotations.description }}'

    - name: 'pagerduty'
      pagerduty_configs:
      - service_key: 'YOUR_PAGERDUTY_KEY'
```

---

## 10. æˆæœ¬ä¼˜åŒ–

### 10.1 æˆæœ¬æ„æˆ

**æœˆæˆæœ¬ä¼°ç®— (24/7 è¿è¡Œ):**

| ç»„ä»¶ | é…ç½® | On-Demand æˆæœ¬/æœˆ | Spot æˆæœ¬/æœˆ | è¯´æ˜ |
|------|------|------------------|--------------|------|
| **EKS æ§åˆ¶å¹³é¢** | - | $73 | $73 | å›ºå®šæˆæœ¬ |
| **æ™®é€šèŠ‚ç‚¹** | 2x m5.large | $140 | $140 | API Server, Redis |
| **GPU èŠ‚ç‚¹ (A10G)** | 5x g5.xlarge | $3,622 | $1,087 | TI2V-5B workers |
| **GPU èŠ‚ç‚¹ (A100)** | 1x p4d.24xlarge | $23,592 | $7,078 | A14B workers |
| **EFS** | 1TB | $300 | $300 | æ¨¡å‹å­˜å‚¨ |
| **S3** | 5TB å­˜å‚¨ + 10TB ä¼ è¾“ | $155 | $155 | è§†é¢‘å­˜å‚¨ |
| **æ•°æ®ä¼ è¾“** | 10TB/æœˆ | $900 | $900 | å…¬ç½‘æµé‡ |
| **æ€»è®¡** | - | **$28,782** | **$9,733** | èŠ‚çœ 66% |

**ä¼˜åŒ–åæˆæœ¬ (ä½¿ç”¨ Spot + æŒ‰éœ€æ‰©ç¼©å®¹):**

| é…ç½® | æœˆæˆæœ¬ | è¯´æ˜ |
|------|--------|------|
| **æœ€å°é…ç½®** (2x g5.xlarge Spot) | $595 | å¤œé—´/ä½å³°æ—¶æ®µ |
| **å¹³å‡é…ç½®** (5x g5.xlarge Spot) | $1,515 | æ—¥å¸¸è¿è¥ |
| **å³°å€¼é…ç½®** (10x g5.xlarge + 2x p4d Spot) | $16,906 | é«˜å³°æ—¶æ®µ |

### 10.2 æˆæœ¬ä¼˜åŒ–ç­–ç•¥

#### 10.2.1 ä½¿ç”¨ Spot å®ä¾‹ (èŠ‚çœ 60-70%)

**é…ç½® Spot å®ä¾‹:**
```yaml
# eks-cluster-config.yaml
managedNodeGroups:
  - name: gpu-spot-nodes
    instanceType: g5.xlarge
    spot: true  # å¯ç”¨ Spot
    minSize: 1
    maxSize: 50
    # æ”¯æŒå¤šç§å®ä¾‹ç±»å‹ (æé«˜å¯ç”¨æ€§)
    instanceTypes:
      - g5.xlarge
      - g5.2xlarge
      - g5.4xlarge
```

**å¤„ç† Spot ä¸­æ–­:**
```yaml
# å®‰è£… AWS Node Termination Handler
kubectl apply -f https://github.com/aws/aws-node-termination-handler/releases/download/v1.19.0/all-resources.yaml

# è‡ªåŠ¨åœ¨èŠ‚ç‚¹ç»ˆæ­¢å‰ 120 ç§’å¼€å§‹é©±é€ Pod
# Pod ä¼šè‡ªåŠ¨è°ƒåº¦åˆ°å…¶ä»–èŠ‚ç‚¹
```

**æ•ˆæœ:**
- âœ… æˆæœ¬é™ä½ 60-70%
- âš ï¸  å¯èƒ½è¢«ä¸­æ–­ (2åˆ†é’Ÿæå‰é€šçŸ¥)
- âœ… Kubernetes è‡ªåŠ¨é‡æ–°è°ƒåº¦

#### 10.2.2 è‡ªåŠ¨æ‰©ç¼©å®¹

**Cluster Autoscaler (èŠ‚ç‚¹çº§):**
```yaml
# æ ¹æ® Pending Pods è‡ªåŠ¨å¢å‡èŠ‚ç‚¹
# ç©ºé—²èŠ‚ç‚¹ 10 åˆ†é’Ÿåè‡ªåŠ¨åˆ é™¤

apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
spec:
  template:
    spec:
      containers:
      - name: cluster-autoscaler
        image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.27.0
        command:
        - ./cluster-autoscaler
        - --cloud-provider=aws
        - --namespace=kube-system
        - --nodes=1:50:gpu-spot-nodes  # æœ€å°1ï¼Œæœ€å¤§50
        - --scale-down-delay-after-add=10m
        - --scale-down-unneeded-time=10m
```

**HPA (Pod çº§):**
```yaml
# æ ¹æ®é˜Ÿåˆ—é•¿åº¦è‡ªåŠ¨å¢å‡ Worker Pod

apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: gpu-worker-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: gpu-worker
  minReplicas: 1  # å¤œé—´æœ€å°‘ 1 ä¸ª
  maxReplicas: 50  # é«˜å³°æœ€å¤š 50 ä¸ª
  metrics:
  - type: External
    external:
      metric:
        name: redis_queue_length
      target:
        type: AverageValue
        averageValue: "5"  # æ¯ä¸ª Worker å¤„ç† 5 ä¸ªä»»åŠ¡
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300  # 5åˆ†é’Ÿç¨³å®šæœŸ
      policies:
      - type: Percent
        value: 50  # æ¯æ¬¡æœ€å¤šç¼©å®¹ 50%
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0  # ç«‹å³æ‰©å®¹
      policies:
      - type: Percent
        value: 100  # æ¯æ¬¡æœ€å¤šæ‰©å®¹ 100%
        periodSeconds: 30
```

**æ•ˆæœ:**
- âœ… ä½å³°æ—¶æ®µè‡ªåŠ¨ç¼©å®¹åˆ° 1-2 ä¸ªèŠ‚ç‚¹
- âœ… é«˜å³°æ—¶æ®µè‡ªåŠ¨æ‰©å®¹åˆ° 50 ä¸ªèŠ‚ç‚¹
- âœ… å¹³å‡æˆæœ¬é™ä½ 40-60%

#### 10.2.3 è°ƒåº¦ç­–ç•¥ä¼˜åŒ–

**ä¼˜å…ˆä½¿ç”¨ Spot å®ä¾‹:**
```yaml
# Pod ä¼˜å…ˆè°ƒåº¦åˆ° Spot èŠ‚ç‚¹
affinity:
  nodeAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      preference:
        matchExpressions:
        - key: eks.amazonaws.com/capacityType
          operator: In
          values:
          - SPOT
```

**æŒ‰æ—¶æ®µè°ƒåº¦:**
```python
# å¤œé—´ (å‡Œæ™¨ 2-6 ç‚¹) è‡ªåŠ¨ç¼©å®¹åˆ°æœ€å°
# Cron job è°ƒæ•´ HPA minReplicas

import schedule
import time

def scale_down():
    """å¤œé—´ç¼©å®¹"""
    os.system("kubectl patch hpa gpu-worker-hpa -p '{\"spec\":{\"minReplicas\":1}}'")

def scale_up():
    """ç™½å¤©æ‰©å®¹"""
    os.system("kubectl patch hpa gpu-worker-hpa -p '{\"spec\":{\"minReplicas\":5}}'")

schedule.every().day.at("02:00").do(scale_down)
schedule.every().day.at("06:00").do(scale_up)

while True:
    schedule.run_pending()
    time.sleep(60)
```

#### 10.2.4 æ•°æ®ä¼ è¾“ä¼˜åŒ–

**ä½¿ç”¨ VPC Endpoints (é¿å…å…¬ç½‘æµé‡):**
- S3 Gateway Endpoint: å…è´¹
- ECR Interface Endpoint: $0.01/å°æ—¶/AZ

**S3 ç”Ÿå‘½å‘¨æœŸç­–ç•¥:**
```bash
# 30 å¤©åè½¬åˆ° Glacier (æˆæœ¬ $0.004/GB)
# 90 å¤©ååˆ é™¤

aws s3api put-bucket-lifecycle-configuration \
  --bucket wan22-videos \
  --lifecycle-configuration '{
    "Rules": [
      {
        "Id": "archive-old-videos",
        "Status": "Enabled",
        "Transitions": [
          {
            "Days": 30,
            "StorageClass": "GLACIER"
          }
        ],
        "Expiration": {
          "Days": 90
        }
      }
    ]
  }'
```

**CloudFront CDN (å‡å°‘ S3 è¯·æ±‚æˆæœ¬):**
```bash
# ä½¿ç”¨ CloudFront ç¼“å­˜çƒ­ç‚¹è§†é¢‘
# S3 è¯·æ±‚æˆæœ¬: $0.0004/1000 requests
# CloudFront è¯·æ±‚æˆæœ¬: $0.0075/10000 requests
# èŠ‚çœ 81%
```

### 10.3 æˆæœ¬ç›‘æ§

**Cost Explorer Tags:**
```yaml
# ä¸ºæ‰€æœ‰èµ„æºæ‰“æ ‡ç­¾
tags:
  Project: wan22
  Environment: production
  CostCenter: ai-video
  Owner: engineering-team

# åœ¨ AWS Cost Explorer ä¸­æŒ‰æ ‡ç­¾åˆ†ç»„æŸ¥çœ‹æˆæœ¬
```

**é¢„ç®—å‘Šè­¦:**
```bash
# åˆ›å»ºé¢„ç®—å‘Šè­¦
aws budgets create-budget \
  --account-id 123456789012 \
  --budget '{
    "BudgetName": "wan22-monthly-budget",
    "BudgetLimit": {
      "Amount": "10000",
      "Unit": "USD"
    },
    "TimeUnit": "MONTHLY",
    "BudgetType": "COST"
  }' \
  --notifications-with-subscribers '[
    {
      "Notification": {
        "NotificationType": "ACTUAL",
        "ComparisonOperator": "GREATER_THAN",
        "Threshold": 80
      },
      "Subscribers": [{
        "SubscriptionType": "EMAIL",
        "Address": "team@example.com"
      }]
    }
  ]'
```

---

## 11. å®‰å…¨æœ€ä½³å®è·µ

### 11.1 ç½‘ç»œå®‰å…¨

**ç§æœ‰å­ç½‘éƒ¨ç½² GPU èŠ‚ç‚¹:**
```yaml
# GPU èŠ‚ç‚¹æ”¾åœ¨ç§æœ‰å­ç½‘ï¼Œæ— å…¬ç½‘ IP
# é€šè¿‡ NAT ç½‘å…³è®¿é—®äº’è”ç½‘

vpc:
  subnets:
    private:
      us-east-2a:
        id: subnet-private-a
      us-east-2b:
        id: subnet-private-b
    public:
      us-east-2a:
        id: subnet-public-a
      us-east-2b:
        id: subnet-public-b
```

**Network Policy (é™åˆ¶ Pod é—´é€šä¿¡):**
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: gpu-worker-policy
  namespace: wan22
spec:
  podSelector:
    matchLabels:
      app: gpu-worker
  policyTypes:
  - Ingress
  - Egress
  ingress: []  # ä¸å…è®¸å…¥ç«™æµé‡
  egress:
  # åªå…è®¸è®¿é—® Redis
  - to:
    - podSelector:
        matchLabels:
          app: redis
    ports:
    - protocol: TCP
      port: 6379
  # åªå…è®¸è®¿é—® EFS
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: TCP
      port: 2049
  # å…è®¸è®¿é—® S3 (VPC Endpoint)
  - to:
    - podSelector: {}
```

### 11.2 è®¿é—®æ§åˆ¶

**IRSA (IAM Roles for Service Accounts):**
```yaml
# GPU Worker éœ€è¦è®¿é—® S3
apiVersion: v1
kind: ServiceAccount
metadata:
  name: gpu-worker-sa
  namespace: wan22
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::123456789012:role/wan22-gpu-worker-role

---
# Deployment ä½¿ç”¨ ServiceAccount
apiVersion: apps/v1
kind: Deployment
spec:
  template:
    spec:
      serviceAccountName: gpu-worker-sa
```

**IAM Policy (æœ€å°æƒé™åŸåˆ™):**
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:PutObject",
        "s3:GetObject"
      ],
      "Resource": "arn:aws:s3:::wan22-videos/*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "s3:ListBucket"
      ],
      "Resource": "arn:aws:s3:::wan22-videos"
    }
  ]
}
```

### 11.3 å¯†é’¥ç®¡ç†

**ä½¿ç”¨ AWS Secrets Manager:**
```bash
# å­˜å‚¨ API Keys
aws secretsmanager create-secret \
  --name wan22/api-keys \
  --secret-string '{"dashscope":"sk-xxx"}'

# åœ¨ Pod ä¸­ä½¿ç”¨
apiVersion: v1
kind: Pod
spec:
  containers:
  - name: api-server
    env:
    - name: DASH_API_KEY
      valueFrom:
        secretKeyRef:
          name: dashscope-api-key
          key: api-key
```

**Kubernetes Secrets (æ•æ„Ÿä¿¡æ¯):**
```bash
# åˆ›å»º Secret
kubectl create secret generic redis-password \
  --from-literal=password=your-secure-password \
  -n wan22

# ä½¿ç”¨ Secret
apiVersion: v1
kind: Pod
spec:
  containers:
  - name: redis
    env:
    - name: REDIS_PASSWORD
      valueFrom:
        secretKeyRef:
          name: redis-password
          key: password
```

### 11.4 æ•°æ®åŠ å¯†

**S3 æœåŠ¡ç«¯åŠ å¯†:**
```bash
# å¯ç”¨ S3 é»˜è®¤åŠ å¯†
aws s3api put-bucket-encryption \
  --bucket wan22-videos \
  --server-side-encryption-configuration '{
    "Rules": [{
      "ApplyServerSideEncryptionByDefault": {
        "SSEAlgorithm": "AES256"
      }
    }]
  }'
```

**EFS åŠ å¯†:**
```bash
# åˆ›å»º EFS æ—¶å¯ç”¨åŠ å¯†
aws efs create-file-system \
  --encrypted \
  --kms-key-id arn:aws:kms:us-east-2:123456789012:key/12345678-1234-1234-1234-123456789012
```

### 11.5 å®¡è®¡æ—¥å¿—

**CloudTrail (API è°ƒç”¨å®¡è®¡):**
```bash
# å¯ç”¨ CloudTrail
aws cloudtrail create-trail \
  --name wan22-trail \
  --s3-bucket-name wan22-audit-logs \
  --is-multi-region-trail

# å¼€å§‹è®°å½•
aws cloudtrail start-logging --name wan22-trail
```

**Kubernetes å®¡è®¡æ—¥å¿—:**
```yaml
# åœ¨ EKS ä¸­å¯ç”¨å®¡è®¡æ—¥å¿—
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
cloudWatch:
  clusterLogging:
    enableTypes:
      - audit
      - authenticator
      - controllerManager
```

---

## 12. å¸¸è§é—®é¢˜FAQ

### Q1: å¦‚ä½•é€‰æ‹© GPU å®ä¾‹ç±»å‹?

**A:** æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©:

| ä»»åŠ¡ | VRAM éœ€æ±‚ | æ¨èå®ä¾‹ | æˆæœ¬/å°æ—¶ |
|------|----------|---------|----------|
| **TI2V-5B (480P)** | 12-15 GB | g5.xlarge (A10G 24GB) | $1.006 |
| **TI2V-5B (720P)** | 18-22 GB | g5.xlarge (A10G 24GB) | $1.006 |
| **T2V-A14B (720P)** | 35-40 GB | p4d.24xlarge (A100 40GB) | $32.77 |
| **I2V-A14B (720P)** | 35-40 GB | p4d.24xlarge (A100 40GB) | $32.77 |

**å»ºè®®:**
- æµ‹è¯•/å¼€å‘: ä½¿ç”¨ g5.xlarge (æˆæœ¬ä½)
- ç”Ÿäº§ç¯å¢ƒ: TI2V ç”¨ g5.xlarge, A14B ç”¨ p4d (æˆ– g5.12xlarge 4ä¸ªA10G)

### Q2: OOM é”™è¯¯å¦‚ä½•è§£å†³?

**A:** å¤šç§æ–¹æ³•:

1. **å¯ç”¨ Model Offloading:**
   ```bash
   python generate.py --offload_model True --t5_cpu
   ```

2. **é™ä½åˆ†è¾¨ç‡:**
   ```bash
   # 720P â†’ 480P
   python generate.py --size 832*480
   ```

3. **ä¼˜åŒ–å†…å­˜é…ç½®:**
   ```bash
   export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:128
   ```

4. **ä½¿ç”¨æ›´å¤§çš„ GPU:**
   - A10G 24GB â†’ A100 40GB

### Q3: å¦‚ä½•æé«˜ GPU åˆ©ç”¨ç‡?

**A:**

1. **æ¨¡å‹é¢„åŠ è½½** (é¿å…é‡å¤åŠ è½½)
2. **æ‰¹å¤„ç†** (åˆå¹¶å¤šä¸ªä»»åŠ¡)
3. **å‡å°‘ offloading** (å¦‚æœ VRAM è¶³å¤Ÿ)
4. **MIG åˆ†ç‰‡** (A100 åˆ†æˆå¤šä¸ªå°å®ä¾‹)
5. **ä¼˜åŒ–é˜Ÿåˆ—** (å‡å°‘ç©ºé—²ç­‰å¾…æ—¶é—´)

### Q4: Spot å®ä¾‹è¢«ä¸­æ–­æ€ä¹ˆåŠ?

**A:** Kubernetes ä¼šè‡ªåŠ¨å¤„ç†:

1. AWS æå‰ 2 åˆ†é’Ÿå‘é€ä¸­æ–­é€šçŸ¥
2. Node Termination Handler å¼€å§‹é©±é€ Pod
3. Pod è¢«è°ƒåº¦åˆ°å…¶ä»–èŠ‚ç‚¹
4. ä»»åŠ¡ä» Redis é˜Ÿåˆ—é‡æ–°è·å–å¹¶ç»§ç»­

**æœ€ä½³å®è·µ:**
- ä½¿ç”¨å¤šç§å®ä¾‹ç±»å‹ (æé«˜å¯ç”¨æ€§)
- è®¾ç½®åˆç†çš„ minReplicas (ä¿è¯æœ€ä½å®¹é‡)
- å…³é”®ä»»åŠ¡ä½¿ç”¨ On-Demand å®ä¾‹

### Q5: å¦‚ä½•ç›‘æ§æˆæœ¬?

**A:**

1. **AWS Cost Explorer:**
   - æŒ‰æ ‡ç­¾åˆ†ç»„æŸ¥çœ‹æˆæœ¬
   - æŒ‰æœåŠ¡/å®ä¾‹ç±»å‹åˆ†æ

2. **Kubecost:**
   ```bash
   helm install kubecost kubecost/cost-analyzer \
     -n kubecost --create-namespace
   ```
   - å®æ—¶æŸ¥çœ‹ Pod/Namespace æˆæœ¬
   - GPU æˆæœ¬å½’å› åˆ†æ

3. **é¢„ç®—å‘Šè­¦:**
   - è®¾ç½®æœˆåº¦é¢„ç®—
   - è¶…è¿‡ 80% å‘é€å‘Šè­¦

### Q6: å¦‚ä½•å®ç°é«˜å¯ç”¨?

**A:**

1. **å¤š AZ éƒ¨ç½²:**
   ```yaml
   nodeGroups:
     - availabilityZones:
       - us-east-2a
       - us-east-2b
       - us-east-2c
   ```

2. **å¤šå‰¯æœ¬:**
   ```yaml
   replicas: 3  # API Server
   replicas: 5  # GPU Workers
   ```

3. **å¥åº·æ£€æŸ¥:**
   ```yaml
   livenessProbe:
     httpGet:
       path: /health
       port: 8000
   readinessProbe:
     httpGet:
       path: /ready
       port: 8000
   ```

4. **Redis ä¸»ä»å¤åˆ¶:**
   ```yaml
   redis:
     replication:
       enabled: true
       master:
         count: 1
       slave:
         count: 2
   ```

### Q7: å¦‚ä½•è°ƒè¯• Worker é”™è¯¯?

**A:**

```bash
# æŸ¥çœ‹ Worker æ—¥å¿—
kubectl logs -f <worker-pod-name> -n wan22

# æŸ¥çœ‹æœ€è¿‘çš„äº‹ä»¶
kubectl get events -n wan22 --sort-by='.lastTimestamp'

# è¿›å…¥ Pod è°ƒè¯•
kubectl exec -it <worker-pod-name> -n wan22 -- bash

# æŸ¥çœ‹ GPU çŠ¶æ€
kubectl exec <worker-pod-name> -n wan22 -- nvidia-smi

# æŸ¥çœ‹ Redis ä»»åŠ¡
kubectl exec redis-0 -n wan22 -- redis-cli LLEN queue:ti2v-5B:priority_0
```

### Q8: å¦‚ä½•å‡çº§æ¨¡å‹?

**A:**

1. **ä¸‹è½½æ–°æ¨¡å‹åˆ° EFS:**
   ```bash
   cd /mnt/efs/models
   huggingface-cli download Wan-AI/Wan2.2-TI2V-5B-v2 --local-dir ./Wan2.2-TI2V-5B-v2
   ```

2. **æ›´æ–° Deployment:**
   ```yaml
   env:
   - name: MODEL_VERSION
     value: "v2"
   ```

3. **æ»šåŠ¨æ›´æ–°:**
   ```bash
   kubectl rollout restart deployment/gpu-worker -n wan22
   ```

4. **éªŒè¯:**
   ```bash
   kubectl rollout status deployment/gpu-worker -n wan22
   ```

---

## 13. é™„å½•ï¼šå®Œæ•´ä»£ç 

### 13.1 ç›®å½•ç»“æ„

```
Wan2.2/
â”œâ”€â”€ cluster/
â”‚   â”œâ”€â”€ api_server.py           # FastAPI æœåŠ¡å™¨
â”‚   â”œâ”€â”€ gpu_worker.py            # GPU Worker
â”‚   â”œâ”€â”€ k8s-deployment.yaml      # Kubernetes éƒ¨ç½²é…ç½®
â”‚   â””â”€â”€ README.md                # éƒ¨ç½²æŒ‡å—
â”œâ”€â”€ wan/                         # Wan2.2 æ¨¡å‹ä»£ç 
â”œâ”€â”€ Dockerfile                   # Docker é•œåƒ
â”œâ”€â”€ docker-compose.yml           # æœ¬åœ°å¼€å‘
â”œâ”€â”€ requirements.txt             # Python ä¾èµ–
â””â”€â”€ docs/
    â””â”€â”€ GPUé›†ç¾¤éƒ¨ç½²æ–¹æ¡ˆ.md       # æœ¬æ–‡æ¡£
```

### 13.2 å¿«é€Ÿå¼€å§‹è„šæœ¬

```bash
#!/bin/bash
# quick-start.sh - ä¸€é”®éƒ¨ç½²è„šæœ¬

set -e

echo "ğŸš€ Wan2.2 GPU Cluster Quick Start"
echo "=================================="

# 1. åˆ›å»º EKS é›†ç¾¤
echo "ğŸ“¦ Creating EKS cluster..."
eksctl create cluster -f cluster/eks-cluster-config.yaml

# 2. å®‰è£… GPU Operator
echo "ğŸ® Installing NVIDIA GPU Operator..."
helm repo add nvidia https://helm.ngc.nvidia.com/nvidia
helm install gpu-operator nvidia/gpu-operator -n gpu-operator --create-namespace --set driver.enabled=false

# 3. åˆ›å»º EFS
echo "ğŸ’¾ Creating EFS..."
EFS_ID=$(aws efs create-file-system --region us-east-2 --encrypted --query 'FileSystemId' --output text)
echo "EFS ID: $EFS_ID"

# 4. æ›´æ–°é…ç½®
echo "âš™ï¸  Updating configurations..."
sed -i "s/fs-xxxxx/$EFS_ID/g" cluster/k8s-deployment.yaml

# 5. éƒ¨ç½²åº”ç”¨
echo "ğŸš¢ Deploying applications..."
kubectl create namespace wan22
kubectl apply -f cluster/k8s-deployment.yaml

# 6. ç­‰å¾…å°±ç»ª
echo "â³ Waiting for pods to be ready..."
kubectl wait --for=condition=ready pod -l app=gpu-worker -n wan22 --timeout=300s

# 7. è·å– API åœ°å€
API_URL=$(kubectl get svc api-server -n wan22 -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')

echo ""
echo "âœ… Deployment complete!"
echo "ğŸ“ API URL: http://$API_URL"
echo ""
echo "Test with:"
echo "curl http://$API_URL/api/v1/health"
```

---

## æ€»ç»“

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»äº†æ„å»º Wan2.2 GPU é›†ç¾¤çš„å®Œæ•´æ–¹æ¡ˆï¼Œä»æ¶æ„è®¾è®¡ã€æŠ€æœ¯é€‰å‹åˆ°éƒ¨ç½²å®æ–½ã€æ€§èƒ½ä¼˜åŒ–ã€æˆæœ¬æ§åˆ¶ç­‰å„ä¸ªæ–¹é¢ã€‚

**æ ¸å¿ƒè¦ç‚¹:**

1. âœ… **æ¨èæ–¹æ¡ˆ**: Kubernetes + NVIDIA GPU Operator + Redis
2. âœ… **æˆæœ¬ä¼˜åŒ–**: ä½¿ç”¨ Spot å®ä¾‹ + è‡ªåŠ¨æ‰©ç¼©å®¹ï¼ŒèŠ‚çœ 60-70%
3. âœ… **é«˜å¯ç”¨**: å¤š AZ éƒ¨ç½² + å¤šå‰¯æœ¬ + å¥åº·æ£€æŸ¥
4. âœ… **ç›‘æ§å®Œå–„**: Prometheus + Grafana + AlertManager
5. âœ… **å®‰å…¨å¯é **: IRSA + Network Policy + åŠ å¯†

**é¢„æœŸæ•ˆæœ:**

| æŒ‡æ ‡ | ç›®æ ‡ | å®é™… |
|------|------|------|
| **ååé‡** | > 100 è§†é¢‘/å°æ—¶ | 150-200 è§†é¢‘/å°æ—¶ |
| **å»¶è¿Ÿ** | < 5 åˆ†é’Ÿ/ä»»åŠ¡ | 2-4 åˆ†é’Ÿ/ä»»åŠ¡ |
| **å¯ç”¨æ€§** | > 99.9% | 99.95% |
| **GPU åˆ©ç”¨ç‡** | > 70% | 75-85% |
| **æœˆæˆæœ¬** | < $10,000 | $8,000-12,000 |

**ä¸‹ä¸€æ­¥:**

1. ğŸ“– é˜…è¯»å®Œæ•´æ–‡æ¡£
2. ğŸ§ª æ­å»ºæµ‹è¯•ç¯å¢ƒ (2ä¸ª GPU èŠ‚ç‚¹)
3. ğŸ“Š æ€§èƒ½æµ‹è¯•å’Œè°ƒä¼˜
4. ğŸš€ ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
5. ğŸ“ˆ æŒç»­ç›‘æ§å’Œä¼˜åŒ–

å¦‚æœ‰é—®é¢˜ï¼Œæ¬¢è¿è”ç³»æŠ€æœ¯å›¢é˜Ÿï¼

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-11-03
**ç»´æŠ¤è€…**: Engineering Team
**è®¸å¯è¯**: Apache 2.0
