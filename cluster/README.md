# Wan2.2 GPU é›†ç¾¤éƒ¨ç½²æŒ‡å—

## ğŸ“‹ ç›®å½•

1. [æ¶æ„æ¦‚è¿°](#æ¶æ„æ¦‚è¿°)
2. [æŠ€æœ¯æ ˆ](#æŠ€æœ¯æ ˆ)
3. [éƒ¨ç½²æ­¥éª¤](#éƒ¨ç½²æ­¥éª¤)
4. [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
5. [ç›‘æ§å‘Šè­¦](#ç›‘æ§å‘Šè­¦)
6. [æˆæœ¬ä¼˜åŒ–](#æˆæœ¬ä¼˜åŒ–)

## ğŸ—ï¸ æ¶æ„æ¦‚è¿°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AWS Application Load Balancer            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              API Server (FastAPI) x 2 replicas              â”‚
â”‚  - æ¥æ”¶ç”¨æˆ·è¯·æ±‚                                              â”‚
â”‚  - ä»»åŠ¡å…¥é˜Ÿ                                                  â”‚
â”‚  - çŠ¶æ€æŸ¥è¯¢                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Redis Cluster (ä»»åŠ¡é˜Ÿåˆ—)                   â”‚
â”‚  - æŒ‰ä»»åŠ¡ç±»å‹åˆ†é˜Ÿåˆ—: ti2v-5B, t2v-A14B, i2v-A14B            â”‚
â”‚  - ä¼˜å…ˆçº§é˜Ÿåˆ—æ”¯æŒ                                            â”‚
â”‚  - ä»»åŠ¡çŠ¶æ€å­˜å‚¨                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GPU Worker Pool                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ TI2V-5B Worker â”‚ TI2V-5B Worker â”‚ TI2V-5B Worker â”‚        â”‚
â”‚  â”‚   (A10G 24GB)  â”‚   (A10G 24GB)  â”‚   (A10G 24GB)  â”‚        â”‚
â”‚  â”‚   g5.xlarge    â”‚   g5.xlarge    â”‚   g5.xlarge    â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚  â”‚ A14B Worker    â”‚ A14B Worker    â”‚                         â”‚
â”‚  â”‚ (A100 40GB)    â”‚ (A100 40GB)    â”‚                         â”‚
â”‚  â”‚ p4d.24xlarge   â”‚ p4d.24xlarge   â”‚                         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       å…±äº«å­˜å‚¨                               â”‚
â”‚  - EFS: æ¨¡å‹æ–‡ä»¶å…±äº« (1TB+)                                 â”‚
â”‚  - S3: ç”Ÿæˆçš„è§†é¢‘è¾“å‡º                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ æŠ€æœ¯æ ˆ

| ç»„ä»¶ | æŠ€æœ¯ | ç”¨é€” |
|------|------|------|
| **å®¹å™¨ç¼–æ’** | Kubernetes (EKS) | GPU èµ„æºè°ƒåº¦ã€è‡ªåŠ¨æ‰©ç¼©å®¹ |
| **GPU ç®¡ç†** | NVIDIA GPU Operator | GPU è®¾å¤‡å‘ç°å’Œç®¡ç† |
| **API ç½‘å…³** | FastAPI | æ¥æ”¶ HTTP è¯·æ±‚ |
| **ä»»åŠ¡é˜Ÿåˆ—** | Redis | ä»»åŠ¡åˆ†å‘å’ŒçŠ¶æ€ç®¡ç† |
| **Worker** | Python + PyTorch | GPU è§†é¢‘ç”Ÿæˆ |
| **å­˜å‚¨** | EFS + S3 | æ¨¡å‹å…±äº« + è§†é¢‘å­˜å‚¨ |
| **ç›‘æ§** | Prometheus + Grafana | ç³»ç»Ÿç›‘æ§ |
| **æ—¥å¿—** | CloudWatch / ELK | æ—¥å¿—èšåˆ |
| **CI/CD** | GitHub Actions | è‡ªåŠ¨éƒ¨ç½² |

## ğŸ“¦ éƒ¨ç½²æ­¥éª¤

### 1. åˆ›å»º EKS é›†ç¾¤

```bash
# ä½¿ç”¨ eksctl åˆ›å»ºé›†ç¾¤
eksctl create cluster \
  --name wan22-cluster \
  --region us-east-2 \
  --version 1.28 \
  --node-type m5.large \
  --nodes 2 \
  --nodes-min 2 \
  --nodes-max 5

# åˆ›å»º GPU èŠ‚ç‚¹ç»„ (A10G)
eksctl create nodegroup \
  --cluster wan22-cluster \
  --region us-east-2 \
  --name gpu-a10g-nodes \
  --node-type g5.xlarge \
  --nodes 2 \
  --nodes-min 1 \
  --nodes-max 10 \
  --node-labels gpu-type=a10g \
  --node-taints nvidia.com/gpu=true:NoSchedule

# åˆ›å»º GPU èŠ‚ç‚¹ç»„ (A100)
eksctl create nodegroup \
  --cluster wan22-cluster \
  --region us-east-2 \
  --name gpu-a100-nodes \
  --node-type p4d.24xlarge \
  --nodes 1 \
  --nodes-min 0 \
  --nodes-max 5 \
  --node-labels gpu-type=a100 \
  --node-taints nvidia.com/gpu=true:NoSchedule
```

### 2. å®‰è£… NVIDIA GPU Operator

```bash
# æ·»åŠ  Helm repo
helm repo add nvidia https://helm.ngc.nvidia.com/nvidia
helm repo update

# å®‰è£… GPU Operator
helm install --wait --generate-name \
  -n gpu-operator --create-namespace \
  nvidia/gpu-operator \
  --set driver.enabled=false  # AWS GPU AMI å·²åŒ…å«é©±åŠ¨

# éªŒè¯ GPU å¯ç”¨
kubectl get nodes "-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\.com/gpu"
```

### 3. åˆ›å»º EFS æ–‡ä»¶ç³»ç»Ÿ

```bash
# ä½¿ç”¨ AWS CLI åˆ›å»º EFS
aws efs create-file-system \
  --region us-east-2 \
  --performance-mode generalPurpose \
  --throughput-mode bursting \
  --encrypted \
  --tags Key=Name,Value=wan22-models

# è®°å½• EFS ID (ä¾‹å¦‚: fs-12345678)
EFS_ID=$(aws efs describe-file-systems --region us-east-2 --query 'FileSystems[?Name==`wan22-models`].FileSystemId' --output text)

# å®‰è£… EFS CSI Driver
kubectl apply -k "github.com/kubernetes-sigs/aws-efs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-1.5"
```

### 4. ä¸Šä¼ æ¨¡å‹åˆ° EFS

```bash
# æŒ‚è½½ EFS åˆ° EC2 å®ä¾‹
sudo mount -t efs -o tls $EFS_ID:/ /mnt/efs

# ä¸‹è½½æ¨¡å‹
mkdir -p /mnt/efs/models
cd /mnt/efs/models

huggingface-cli download Wan-AI/Wan2.2-TI2V-5B --local-dir ./Wan2.2-TI2V-5B
huggingface-cli download Wan-AI/Wan2.2-T2V-A14B --local-dir ./Wan2.2-T2V-A14B
huggingface-cli download Wan-AI/Wan2.2-I2V-A14B --local-dir ./Wan2.2-I2V-A14B
```

### 5. éƒ¨ç½²æœåŠ¡

```bash
# æ›´æ–° k8s-deployment.yaml ä¸­çš„ EFS ID
sed -i "s/fs-xxxxx/$EFS_ID/g" cluster/k8s-deployment.yaml

# åˆ›å»ºå‘½åç©ºé—´
kubectl create namespace wan22

# éƒ¨ç½²æ‰€æœ‰ç»„ä»¶
kubectl apply -f cluster/k8s-deployment.yaml

# æŸ¥çœ‹éƒ¨ç½²çŠ¶æ€
kubectl get pods -n wan22 -w
```

### 6. è·å– API åœ°å€

```bash
# è·å– LoadBalancer åœ°å€
kubectl get svc api-server -n wan22

# æµ‹è¯• API
API_URL=$(kubectl get svc api-server -n wan22 -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')

curl http://$API_URL/api/v1/health
```

## ğŸ¯ ä½¿ç”¨ç¤ºä¾‹

### æäº¤è§†é¢‘ç”Ÿæˆä»»åŠ¡

```bash
curl -X POST http://$API_URL/api/v1/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "A cat playing piano in a jazz club",
    "resolution": "1280*704",
    "task_type": "ti2v-5B",
    "priority": 0
  }'

# å“åº”
{
  "task_id": "abc123-def456-...",
  "status": "queued",
  "status_url": "/api/v1/status/abc123-def456-..."
}
```

### æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€

```bash
curl http://$API_URL/api/v1/status/abc123-def456-...

# å“åº”
{
  "task_id": "abc123-def456-...",
  "status": "completed",
  "progress": 100,
  "result_url": "https://s3.amazonaws.com/...",
  "created_at": "2025-11-03T10:00:00Z",
  "updated_at": "2025-11-03T10:05:30Z"
}
```

## âš¡ æ€§èƒ½ä¼˜åŒ–

### 1. GPU åˆ©ç”¨ç‡ä¼˜åŒ–

**æ‰¹å¤„ç† (Batching)**
```python
# åœ¨ gpu_worker.py ä¸­å®ç°åŠ¨æ€æ‰¹å¤„ç†
class GPUWorker:
    def process_batch(self, tasks):
        """æ‰¹é‡å¤„ç†å¤šä¸ªä»»åŠ¡"""
        prompts = [task['prompt'] for task in tasks]

        # æ‰¹é‡ç”Ÿæˆ (å‡å°‘æ¨¡å‹åŠ è½½å¼€é”€)
        videos = model.generate_batch(prompts)

        for video, task in zip(videos, tasks):
            self.upload_to_s3(video, task['task_id'])
```

**æ¨¡å‹é¢„åŠ è½½**
```python
# Worker å¯åŠ¨æ—¶é¢„åŠ è½½æ‰€æœ‰æ¨¡å‹
worker.load_model("ti2v-5B")
worker.load_model("t2v-A14B")
```

### 2. å†…å­˜ä¼˜åŒ–

```yaml
# ä½¿ç”¨ MIG åˆ†å‰² A100 (å¤šç§Ÿæˆ·)
apiVersion: v1
kind: ConfigMap
metadata:
  name: mig-config
data:
  config.yaml: |
    version: v1
    mig-configs:
      all-3g.20gb:
        - devices: [0]
          mig-devices:
            3g.20gb: 3  # å°†1ä¸ªA100åˆ†æˆ3ä¸ª20GBå®ä¾‹
```

### 3. ç½‘ç»œä¼˜åŒ–

```yaml
# ä½¿ç”¨ NodeLocal DNSCache åŠ é€Ÿ DNS æŸ¥è¯¢
# ä½¿ç”¨ VPC Endpoints è®¿é—® S3/ECRï¼Œé¿å…å…¬ç½‘æµé‡
```

## ğŸ“Š ç›‘æ§å‘Šè­¦

### å®‰è£… Prometheus + Grafana

```bash
# æ·»åŠ  Helm repo
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

# å®‰è£… Prometheus
helm install prometheus prometheus-community/kube-prometheus-stack \
  -n monitoring --create-namespace

# è®¿é—® Grafana
kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80
# ç”¨æˆ·å: admin, å¯†ç : prom-operator
```

### å…³é”®æŒ‡æ ‡

| æŒ‡æ ‡ | é˜ˆå€¼ | å‘Šè­¦ |
|------|------|------|
| GPU åˆ©ç”¨ç‡ | < 60% | èµ„æºæµªè´¹ |
| é˜Ÿåˆ—é•¿åº¦ | > 100 | éœ€è¦æ‰©å®¹ |
| ä»»åŠ¡å¤±è´¥ç‡ | > 5% | æœåŠ¡å¼‚å¸¸ |
| API å“åº”æ—¶é—´ | > 500ms | æ€§èƒ½ä¸‹é™ |
| Worker é‡å¯æ¬¡æ•° | > 3/hour | OOM æˆ–å´©æºƒ |

## ğŸ’° æˆæœ¬ä¼˜åŒ–

### 1. ä½¿ç”¨ Spot å®ä¾‹ (èŠ‚çœ 70%)

```bash
eksctl create nodegroup \
  --cluster wan22-cluster \
  --name gpu-spot-nodes \
  --node-type g5.xlarge \
  --spot \
  --instance-types g5.xlarge,g5.2xlarge \
  --nodes-min 0 \
  --nodes-max 20
```

### 2. æŒ‰éœ€æ‰©ç¼©å®¹

```yaml
# Cluster Autoscaler - æ ¹æ®è´Ÿè½½è‡ªåŠ¨å¢å‡èŠ‚ç‚¹
# HPA - æ ¹æ®é˜Ÿåˆ—é•¿åº¦è‡ªåŠ¨å¢å‡ Pod
# Keda - æ›´é«˜çº§çš„è‡ªåŠ¨æ‰©ç¼©å®¹
```

### 3. æˆæœ¬é¢„ä¼°

| é…ç½® | å®ä¾‹ç±»å‹ | æˆæœ¬/å°æ—¶ | GPUæ•° | å»ºè®®ç”¨é€” |
|------|---------|----------|-------|---------|
| **TI2V-5B** | g5.xlarge | $1.006 | 1x A10G | 480P/720P è§†é¢‘ |
| **TI2V-5B** | g5.2xlarge | $1.212 | 1x A10G | åŒä¸Šï¼Œæ›´å¤šCPUå†…å­˜ |
| **A14B** | p4d.24xlarge | $32.77 | 8x A100 | 720P é«˜è´¨é‡è§†é¢‘ |
| **A14B** | g5.12xlarge | $5.672 | 4x A10G | å¤šä»»åŠ¡å¹¶è¡Œ |

**æœˆæˆæœ¬ä¼°ç®— (24/7 è¿è¡Œ):**
- 2x g5.xlarge (TI2V): ~$1,450/æœˆ
- 1x p4d.24xlarge (A14B): ~$23,600/æœˆ
- **Spot å®ä¾‹å¯èŠ‚çœ 70%**: ~$7,500/æœˆ

## ğŸ” å®‰å…¨æœ€ä½³å®è·µ

1. **IRSA**: ä½¿ç”¨ IAM Roles for Service Accounts
2. **Secrets**: ä½¿ç”¨ AWS Secrets Manager å­˜å‚¨ API Keys
3. **Network Policy**: é™åˆ¶ Pod é—´é€šä¿¡
4. **ç§æœ‰å­ç½‘**: GPU èŠ‚ç‚¹æ”¾åœ¨ç§æœ‰å­ç½‘
5. **S3 åŠ å¯†**: å¯ç”¨ S3 Server-Side Encryption

## ğŸš€ å…¶ä»–æ¨èæ–¹æ¡ˆ

### æ–¹æ¡ˆ B: Ray on Kubernetes

```python
# ä½¿ç”¨ Ray Serve æ›´ç®€å•çš„éƒ¨ç½²
from ray import serve

@serve.deployment(num_replicas=3, ray_actor_options={"num_gpus": 1})
class Wan22Deployment:
    def __init__(self):
        self.model = load_model()

    async def __call__(self, request):
        return await self.model.generate(request.prompt)

serve.run(Wan22Deployment.bind())
```

### æ–¹æ¡ˆ C: SageMaker + Lambda

- ä½¿ç”¨ SageMaker Async Inference
- Lambda å¤„ç† API è¯·æ±‚
- æˆæœ¬æ›´ä½ï¼Œä½†å†·å¯åŠ¨è¾ƒæ…¢

## ğŸ“š å‚è€ƒèµ„æ–™

- [NVIDIA GPU Operator æ–‡æ¡£](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/getting-started.html)
- [EKS GPU æœ€ä½³å®è·µ](https://docs.aws.amazon.com/eks/latest/userguide/gpu-ami.html)
- [Ray åˆ†å¸ƒå¼è®¡ç®—](https://docs.ray.io/en/latest/)
- [Kubernetes HPA](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)

---

**æ€»ç»“**: æ¨èä½¿ç”¨ **Kubernetes + Redis + GPU Workers** æ–¹æ¡ˆï¼Œç»“åˆ Spot å®ä¾‹å’Œè‡ªåŠ¨æ‰©ç¼©å®¹ï¼Œåœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶æ§åˆ¶æˆæœ¬ã€‚
