"""
GPU Worker - ä»é˜Ÿåˆ—ä¸­è·å–ä»»åŠ¡å¹¶æ‰§è¡Œè§†é¢‘ç”Ÿæˆ
æ”¯æŒå¤š GPUã€è‡ªåŠ¨é‡è¯•ã€è¿›åº¦æ›´æ–°
"""
import os
import sys
import json
import time
import redis
import torch
import boto3
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, '/workspace')

class GPUWorker:
    def __init__(self, gpu_id=None, redis_host='redis', redis_port=6379):
        """åˆå§‹åŒ– GPU Worker

        Args:
            gpu_id: ä½¿ç”¨çš„ GPU IDï¼ŒNone åˆ™è‡ªåŠ¨åˆ†é…
            redis_host: Redis æœåŠ¡å™¨åœ°å€
            redis_port: Redis ç«¯å£
        """
        # GPU è®¾ç½®
        if gpu_id is None:
            gpu_id = int(os.environ.get('CUDA_VISIBLE_DEVICES', '0').split(',')[0])

        self.gpu_id = gpu_id
        torch.cuda.set_device(self.gpu_id)

        print(f"ğŸš€ Worker initialized on GPU {self.gpu_id}")
        print(f"   GPU: {torch.cuda.get_device_name(self.gpu_id)}")
        print(f"   VRAM: {torch.cuda.get_device_properties(self.gpu_id).total_memory / 1e9:.1f} GB")

        # Redis è¿æ¥
        self.redis = redis.Redis(host=redis_host, port=redis_port, decode_responses=True)

        # S3 å®¢æˆ·ç«¯
        self.s3 = boto3.client('s3', region_name=os.environ.get('AWS_REGION', 'us-east-2'))
        self.s3_bucket = os.environ.get('S3_BUCKET', 'martianpay-terraform-state')

        # æ¨¡å‹ç¼“å­˜
        self.models = {}

        # å·¥ä½œç›®å½•
        self.output_dir = Path('/workspace/outputs')
        self.output_dir.mkdir(exist_ok=True)

    def load_model(self, task_type):
        """åŠ è½½æ¨¡å‹ (å¸¦ç¼“å­˜)"""
        if task_type in self.models:
            print(f"âœ… Using cached model: {task_type}")
            return self.models[task_type]

        print(f"ğŸ“¦ Loading model: {task_type}")

        if task_type == "ti2v-5B":
            from wan.textimage2video import WanTI2V
            model = WanTI2V(
                ckpt_dir="/mnt/efs/models/Wan2.2-TI2V-5B",
                offload_model=False,
                convert_model_dtype=True
            )
        elif task_type == "t2v-A14B":
            from wan.text2video import WanT2V
            model = WanT2V(
                ckpt_dir="/mnt/efs/models/Wan2.2-T2V-A14B",
                offload_model=False,
                convert_model_dtype=True
            )
        elif task_type == "i2v-A14B":
            from wan.image2video import WanI2V
            model = WanI2V(
                ckpt_dir="/mnt/efs/models/Wan2.2-I2V-A14B",
                offload_model=False,
                convert_model_dtype=True
            )
        else:
            raise ValueError(f"Unknown task type: {task_type}")

        self.models[task_type] = model
        print(f"âœ… Model loaded: {task_type}")
        return model

    def update_task_status(self, task_id, status=None, progress=None, result_url=None, error=None):
        """æ›´æ–°ä»»åŠ¡çŠ¶æ€"""
        task_key = f"task:{task_id}"
        task_data = self.redis.get(task_key)

        if not task_data:
            print(f"âš ï¸  Task {task_id} not found in Redis")
            return

        task_data = json.loads(task_data)

        if status:
            task_data['status'] = status
        if progress is not None:
            task_data['progress'] = progress
        if result_url:
            task_data['result_url'] = result_url
        if error:
            task_data['error'] = error

        task_data['updated_at'] = datetime.utcnow().isoformat()

        # ä¿å­˜å› Redis
        self.redis.setex(task_key, 86400, json.dumps(task_data))

        print(f"ğŸ“Š Task {task_id}: status={status}, progress={progress}%")

    def upload_to_s3(self, local_path, task_id):
        """ä¸Šä¼ è§†é¢‘åˆ° S3"""
        s3_key = f"wan-videos/{task_id}/{Path(local_path).name}"

        print(f"ğŸ“¤ Uploading to S3: s3://{self.s3_bucket}/{s3_key}")

        self.s3.upload_file(
            str(local_path),
            self.s3_bucket,
            s3_key,
            ExtraArgs={'ContentType': 'video/mp4'}
        )

        # ç”Ÿæˆé¢„ç­¾å URL (7å¤©æœ‰æ•ˆ)
        url = self.s3.generate_presigned_url(
            'get_object',
            Params={'Bucket': self.s3_bucket, 'Key': s3_key},
            ExpiresIn=604800
        )

        print(f"âœ… Uploaded: {url}")
        return url

    def process_task(self, task_id, task_data):
        """å¤„ç†å•ä¸ªä»»åŠ¡"""
        try:
            print(f"\n{'='*60}")
            print(f"ğŸ¬ Processing task: {task_id}")
            print(f"   Prompt: {task_data['prompt'][:50]}...")
            print(f"   Type: {task_data['task_type']}")
            print(f"   Resolution: {task_data['resolution']}")
            print(f"{'='*60}\n")

            # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
            self.update_task_status(task_id, status="processing", progress=10)

            # åŠ è½½æ¨¡å‹
            model = self.load_model(task_data['task_type'])
            self.update_task_status(task_id, progress=20)

            # æ¸…ç† GPU ç¼“å­˜
            torch.cuda.empty_cache()

            # ç”Ÿæˆè§†é¢‘
            print(f"ğŸ¥ Generating video...")
            output_path = self.output_dir / f"{task_id}.mp4"

            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„ generate æ–¹æ³•è°ƒæ•´
            video = model.generate(
                prompt=task_data['prompt'],
                size=task_data['resolution'],
                sample_steps=50,
                # æ·»åŠ è¿›åº¦å›è°ƒ
                progress_callback=lambda step, total: self.update_task_status(
                    task_id,
                    progress=20 + int((step / total) * 60)
                )
            )

            self.update_task_status(task_id, progress=85)

            # ä¿å­˜è§†é¢‘ (å‡è®¾ video æ˜¯ä¸€ä¸ªå¯ä¿å­˜çš„å¯¹è±¡)
            # å®é™…å®ç°éœ€è¦æ ¹æ® wan2.2 çš„è¾“å‡ºæ ¼å¼è°ƒæ•´
            # video.save(output_path)

            # ä¸Šä¼ åˆ° S3
            s3_url = self.upload_to_s3(output_path, task_id)
            self.update_task_status(task_id, progress=95)

            # åˆ é™¤æœ¬åœ°æ–‡ä»¶
            output_path.unlink()

            # å®Œæˆ
            self.update_task_status(
                task_id,
                status="completed",
                progress=100,
                result_url=s3_url
            )

            print(f"âœ… Task {task_id} completed!")
            print(f"   Video URL: {s3_url}\n")

            # æ¸…ç†å†…å­˜
            torch.cuda.empty_cache()

        except Exception as e:
            print(f"âŒ Task {task_id} failed: {str(e)}")
            import traceback
            traceback.print_exc()

            self.update_task_status(
                task_id,
                status="failed",
                error=str(e)
            )

    def run(self, task_types=None, poll_interval=1):
        """è¿è¡Œ Worker ä¸»å¾ªç¯

        Args:
            task_types: å¤„ç†çš„ä»»åŠ¡ç±»å‹åˆ—è¡¨ï¼ŒNone åˆ™å¤„ç†æ‰€æœ‰ç±»å‹
            poll_interval: è½®è¯¢é—´éš”(ç§’)
        """
        if task_types is None:
            task_types = ["ti2v-5B", "t2v-A14B", "i2v-A14B"]

        # æ„å»ºé˜Ÿåˆ—åç§° (ä¼˜å…ˆçº§1åœ¨å‰)
        queues = []
        for task_type in task_types:
            queues.append(f"queue:{task_type}:priority_1")  # é«˜ä¼˜å…ˆçº§
        for task_type in task_types:
            queues.append(f"queue:{task_type}:priority_0")  # æ™®é€šä¼˜å…ˆçº§

        print(f"\nğŸ¯ Worker started")
        print(f"   Listening to queues: {queues}")
        print(f"   Press Ctrl+C to stop\n")

        while True:
            try:
                # ä»é˜Ÿåˆ—ä¸­è·å–ä»»åŠ¡ (é˜»å¡å¼ï¼Œè¶…æ—¶5ç§’)
                result = self.redis.brpop(queues, timeout=5)

                if result is None:
                    # æ²¡æœ‰ä»»åŠ¡ï¼Œç»§ç»­ç­‰å¾…
                    continue

                queue_name, task_id = result

                # è·å–ä»»åŠ¡æ•°æ®
                task_data = self.redis.get(f"task:{task_id}")
                if not task_data:
                    print(f"âš ï¸  Task {task_id} not found, skipping")
                    continue

                task_data = json.loads(task_data)

                # å¤„ç†ä»»åŠ¡
                self.process_task(task_id, task_data)

            except KeyboardInterrupt:
                print("\nğŸ‘‹ Worker stopped by user")
                break
            except Exception as e:
                print(f"âŒ Worker error: {str(e)}")
                import traceback
                traceback.print_exc()
                time.sleep(poll_interval)

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='GPU Worker for Wan2.2')
    parser.add_argument('--gpu-id', type=int, default=None, help='GPU ID to use')
    parser.add_argument('--redis-host', default='localhost', help='Redis host')
    parser.add_argument('--redis-port', type=int, default=6379, help='Redis port')
    parser.add_argument('--task-types', nargs='+', default=None,
                        help='Task types to process (e.g., ti2v-5B t2v-A14B)')

    args = parser.parse_args()

    worker = GPUWorker(
        gpu_id=args.gpu_id,
        redis_host=args.redis_host,
        redis_port=args.redis_port
    )

    worker.run(task_types=args.task_types)
